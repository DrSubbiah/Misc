{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIWrlysaMxvmdfU5cq4geK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrSubbiah/Misc/blob/main/n_1_in_sample_variance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"maroon\"> **An Explanation of the $n-1$ Term in Sample Variance Variance**\n",
        "\n",
        "It may sometimes be a surprise when we see $n-1$ in the denominator of the sample variance computed from a sample $x_1,x_2,x_3,\\cdots,x_n$ of size $n$.\n",
        "\n",
        "This is a natural way of addressing the statistical desire of obtaining an **unbiased estimator $W(X)$** of a parameter $\\theta$.\n",
        "\n",
        "In fact, bias $=E[W(X)]-\\theta$ is a more intuitive quantity that one should pursue, rather than looking at it as a simple or direct mathematical *computation*.\n",
        "\n",
        "However, it may be considered a good practice or even a culture in statistics to derive the properties of $W(X)$ in closed form, wherever possible.\n",
        "\n",
        "One such case is the sample mean and variance. Many authors include systematic discussions of this in their books under the treatment of *Mathematical Statistics*. For example, Casella and Berger (2002) have included this topic in their chapter *Properties of a Random Sample*. In particular, see Section 5.2 (p. 211) and the paragraph on p. 214 following the proof of Theorem 5.2.6, although bias of an estimator is systematically defined in a later chapter.\n",
        "\n",
        "Perhaps, this property **as per their definition of sample variance** with $n-1$ might help the reader connect the concepts of random sample, statistic, parameter, estimation, and its quality in later chapters.\n",
        "\n",
        "\n",
        "\n",
        "## <font color=\"darkblue\"> **A Model Based Approach**\n",
        "\n",
        "Usually, the sample mean is defined as the sum of all sample observations divided by the sample size. Symbolically, it is\n",
        "$$\n",
        "\\bar{X}=\\frac{\\sum_i x_i}{n}.\n",
        "$$\n",
        "\n",
        "This can be understood from a distributional point of view as random samples from a uniform distribution and the corresponding expectation.\n",
        "\n",
        "Equally, this quantity can be understood as a null model arising from fitting a linear model. Further, assuming normally distributed errors $\\epsilon$ with variance $\\sigma^2$, one may refer to Montgomery et al. (2012) for the estimation of $\\sigma^2$ using a form of sample variance.\n",
        "\n",
        "Indeed, the book discusses this idea in two chapters: simple (Section 2.2.3, p. 20) and multiple (Section 3.2.4, p. 80) linear regression models. If $p$ is the number of predictors in the model, including the intercept, the formula for the ***Residual or Error Sum of Squares*** is\n",
        "$$\n",
        "\\frac{\\sum (y_i-\\hat{y}_i)^2}{n-p}.\n",
        "$$\n",
        "\n",
        "Thus, in this language, a null model has $p=1$, and hence the sample variance has $n-1$ in its denominator. It is evident that $\\hat{y}_i$ is simply the sample mean. Also, note that $n-p$ guarantees the **unbiasedness** of the estimator.\n",
        "\n",
        "Additionally, Agresti (2015, p. 50) makes an important remark on the proper usage of terminology when naming a quantity, explicitly mentioning the difference between the ***Residual or Error Sum of Squares*** and the ***Mean Squared Error*** of an estimator $W(X)$.\n",
        "\n",
        "\n",
        "## <font color=\"blue\"> **References**\n",
        "Casella, G., & Berger, R. (2002). Statistical inference. Chapman and Hall/CRC.\n",
        "\n",
        "Montgomery, D. C., Peck, E. A., & Vining, G. G. (2012). Introduction to linear regression analysis. John Wiley & Sons.\n",
        "\n",
        "Agresti, A. (2015). Foundations of linear and generalized linear models. John Wiley & Sons.\n",
        "\n",
        "### <font color=\"magenta\"> **A Friendly Read**\n",
        "\n",
        "A nice handout for some mathematical treatment of this idea.\n",
        "\n",
        "https://math.oxford.emory.edu/site/math117/besselCorrection/"
      ],
      "metadata": {
        "id": "fBS6tIKJyuo6"
      }
    }
  ]
}