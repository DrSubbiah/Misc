{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPYYKlBI5sPL"
   },
   "source": [
    "<span style=\"font-size:15px\">\n",
    "    \n",
    "# <font color = maroon> Fitting Linear Model\n",
    "\n",
    "    \n",
    "## <font color = blue>Points to Ponder\n",
    "\n",
    "- Major aim is to understand ways to build a mathematical relation / mapping from a contextual possible relationships between quantities involved in a real time situation\n",
    "    \n",
    "\n",
    "- It is possible to construct different forms of such mapping in an explicit mathematical form \n",
    "    \n",
    "\n",
    "- Liner Model (LM) - A straightline (Linear) mapping is an easiest way to build mappings / models. However, it is possible to go beyond such modeling to construct Non-Linear (Curvilinear) models\n",
    "    \n",
    "\n",
    "- Symbollically, such  models are represented as $Y=f(X)$ where $Y$ is the response variable and $X$ is a set of predictors identified through a contextual mapping from a study / data. $f$ is the relation / mapping /  function between $X$ and $Y$. Generally, such practices are termed as Modeling\n",
    "    \n",
    "\n",
    "- There are two major types of modeling, depends majorly on the objective of the problem at hand; either to develop model  to **understand (quantify)** the possible relationship between $x$ and $Y$ or the interest is **only to predict** value of $Y$ for a given $X$. Thi notes confines to the first modeling practice - interpretable or explainable model\n",
    "    \n",
    "\n",
    "- In real time situations (predominantly random, unpredictable events), this mapping includes a component of error / noise in the data; the actual model is $Y=f(X)+\\textrm{Error}(\\epsilon)$.\n",
    "    \n",
    "\n",
    "- If we assume, $k$ is the number of predictors then the form of Linear model is \n",
    "$$E[Y|X] = \\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdot\\cdot\\cdot\\cdot+\\beta_kX_k+\\epsilon$$, where\n",
    "$\\beta's$ are **weights / coefficients / betas** that are to be **estimated** (not  to **determine**) from the data\n",
    "Errors $(\\epsilon)$ are random and assumed to follow a Normal distribution with mean zero and variance $\\sigma^2$\n",
    "    \n",
    "\n",
    "- Methods to build LM (and other models) is **highly dependent on the nature of the response variable**. Here, we assume the response variable as numeric (measurable, continuous) one\n",
    "\n",
    "<hr style=\"border: 1.5px solid blue\"/>\n",
    "\n",
    "#### Necessary Symbols:\n",
    "\n",
    "- k: Number of predictors\n",
    "    \n",
    "    \n",
    "- p = k + 1 (number of predictors + constant term)\n",
    "    \n",
    "    \n",
    "- X: The model matrix of predictors, including the constant term, is represented as:  \n",
    "$$X = \\begin{pmatrix}\n",
    "  x_{ij} \\end{pmatrix}_{n\\times p}$$  \n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "  x_{10} & x_{11} & \\cdots & x_{1k} \\\\\n",
    "  x_{20} & x_{21} & \\cdots & x_{2k} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  x_{n0} & x_{n1} & \\cdots & x_{nk}\n",
    "\\end{pmatrix}_{n \\times p}\n",
    "$$ where $x_{i0} = 1 ~~\\forall i = 1,2\\cdots n$, representing the intercept term (constant term) in the linear model\n",
    "and $x_{ij} $ for $j = 1, 2, 3, \\dots, k$ are the values of the predictor variables for each of the $n$ data points.\n",
    "\n",
    "    \n",
    "\n",
    "- Y: Response variable \n",
    "$$Y =\n",
    " \\begin{pmatrix}\n",
    "  y_{1} & y_{2} & \\cdots & y_{n} \n",
    " \\end{pmatrix}^T_{n\\times 1}$$\n",
    "\n",
    "    \n",
    "- Mean Response $\\mu = E[Y]$\n",
    "$$\\mu =\n",
    " \\begin{pmatrix}\n",
    "  \\mu_{1} & \\mu_{2} & \\cdots & \\mu_{n} \n",
    " \\end{pmatrix}^T_{n\\times 1}$$ with $\\mu_i=E[y_i]$\n",
    " \n",
    "    \n",
    "- $\\beta$: Parameter matrix \n",
    " $$\\beta =\n",
    " \\begin{pmatrix}\n",
    "  \\beta_{1} & \\beta_{2} &   \\cdots  & \\beta_{p} \n",
    " \\end{pmatrix}^T_{p\\times 1}$$ \n",
    " \n",
    "    \n",
    "- $\\epsilon$: Random error matrix. The errors are assumed to follow Normal distribution with mean zero and unknown variance $\\sigma^2$; also, it is assumed that the errors are uncorrelated.\n",
    " $$\\epsilon =\n",
    " \\begin{pmatrix}\n",
    "  \\epsilon_{1} &  \\epsilon_{2} &  \\cdots & \\epsilon_{n} \n",
    " \\end{pmatrix}^T_{n\\times 1}$$ \n",
    "\n",
    "\n",
    "    \n",
    "A Linear Model is $$Y_{n \\times 1}=X_{n \\times p}\\beta\\,+\\,\\epsilon_{n \\times 1}$$\n",
    "    \n",
    "$$ \\implies \n",
    " \\begin{pmatrix}\n",
    "  y_{1} \\\\\n",
    "  y_{2} \\\\\n",
    "  \\vdots  \\\\\n",
    "  y_{n} \n",
    " \\end{pmatrix}_{n\\times 1}=\\begin{pmatrix}\n",
    "  x_{10} & x_{11} & \\cdots & x_{1k} \\\\\n",
    "  x_{20} & x_{21} & \\cdots & x_{2k} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  x_{n0} & x_{n1} & \\cdots & x_{nk}\n",
    "\\end{pmatrix}_{n \\times p}\\,\n",
    " \\begin{pmatrix}\n",
    "  \\beta_{0} \\\\\n",
    "  \\beta_{1} \\\\\n",
    "  \\vdots  \\\\\n",
    "  \\beta_{k} \n",
    " \\end{pmatrix}_{p\\times 1}+\\begin{pmatrix}\n",
    "  \\epsilon_{1} \\\\\n",
    "  \\epsilon_{2} \\\\\n",
    "  \\vdots  \\\\\n",
    "  \\epsilon_{n} \n",
    " \\end{pmatrix}_{n\\times 1}$$ \n",
    " \n",
    " $$=\\begin{pmatrix}\n",
    " x_{10}\\beta_{0}+x_{11}\\beta_{1}+x_{12}\\beta_{2}+\\cdots\\cdots x_{1k}\\beta_{k}\\\\\n",
    " x_{20}\\beta_{0}+x_{21}\\beta_{1}+x_{22}\\beta_{2}+\\cdots\\cdots x_{2k}\\beta_{k}\\\\\n",
    " \\vdots \\\\\n",
    " x_{n0}\\beta_{0}+x_{n1}\\beta_{1}+x_{n2}\\beta_{2}+\\cdots\\cdots x_{nk}\\beta_{k}\n",
    " \\end{pmatrix}_{n \\times 1}+\\begin{pmatrix}\n",
    "  \\epsilon_{1} \\\\\n",
    "  \\epsilon_{2} \\\\\n",
    "  \\vdots  \\\\\n",
    "  \\epsilon_{n} \n",
    " \\end{pmatrix}_{n\\times 1}$$\n",
    " \n",
    " $$=\\begin{pmatrix}\n",
    " \\beta_{0}+x_{11}\\beta_{1}+x_{12}\\beta_{2}+\\cdots\\cdots x_{1k}\\beta_{k}+\\epsilon_1\\\\\n",
    " \\beta_{0}+x_{21}\\beta_{1}+x_{22}\\beta_{2}+\\cdots\\cdots x_{2k}\\beta_{k}+\\epsilon_2\\\\\n",
    " \\vdots \\\\\n",
    " \\beta_{0}+x_{n1}\\beta_{1}+x_{n2}\\beta_{2}+\\cdots\\cdots x_{nk}\\beta_{k}+\\epsilon_n\n",
    " \\end{pmatrix}_{n \\times 1}$$ since $x_{i0}=1 \\,\\, \\forall i = 1,2,3,\\cdots\\cdots n$\n",
    "\n",
    "\n",
    "$$\\implies \\mu = E(Y|X) = \\sum_{j=0}^k \\beta_j\\,x_{ij}$$ and $$ V(Y|X)=\\sigma^2 $$. \n",
    "    \n",
    "Hence, in the linear model, we estimate the mean of the response variable $Y$ as a linear function of the predictors \n",
    "$X$, and not the exact value of $Y$\n",
    "\n",
    "<hr style=\"border: 1.5px solid blue\"/>\n",
    "    \n",
    "## <font color = darkgreen> Aim: Parameter estimation ($\\hat \\beta$) \n",
    "    \n",
    "The notion of Least Square Method (LSM) is to minimize the value of $||Y-\\hat \\mu||^2$\n",
    "\n",
    "\n",
    "$$||Y-\\hat \\mu||^2 = \\sum_{i=1}^{n}(y_i-\\hat\\mu_i)^2= \\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}\\hat\\beta_jx_{ij})^2$$\n",
    "\n",
    "Similar expression can be obtained from the notion of MLE when Our assumption is assume that $\\forall i = 1,2,\\cdots n~~~~ y_i\\sim \\text{Normal}(\\mu_i,\\sigma^2)$. \n",
    "\n",
    "$\\Rightarrow$ the log likelihood (excluding constant) is $$l(\\beta|y) = -\\frac{\\sum_i (y_i-\\mu_i)^2}{2\\sigma^2}$$. Hence, maximizing likelihood is equivalent to minimize the $\\sum_i (y_i-\\mu_i)^2$\n",
    "\n",
    "Hence $l(\\beta) = \\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}\\beta_jx_{ij})^2$ yields p equations while we equate $\\frac{\\partial~ l }{\\partial~ \\beta_j}$ to zero $\\forall j=1,2,\\cdots p$\n",
    "\n",
    "That is $\\sum_i(y_i-\\mu_i)x_{ij}=0 ~~~~~\\forall j=1,2,\\cdots p$\n",
    "\n",
    "Hence the LS estimates satisfy $\\sum_iy_ix_{ij}=\\sum_i\\hat\\mu_ix_{ij}~~~~~\\forall j=1,2,\\cdots p$\n",
    "\n",
    "#### Matrix Notation \n",
    "\n",
    "$l(\\beta)=||y-X\\beta||^2=(y-X\\beta)^T(y-X\\beta)$\n",
    "\n",
    "=$(y^T-\\beta^TX^T)(y-X\\beta)$\n",
    "\n",
    "=$y^Ty-y^TX\\beta-\\beta^TX^Ty+\\beta^TX^TX\\beta$. It can be observed that $y^TX\\beta$ is a constant and hence $\\beta^TX^Ty=(y^TX\\beta)^T=y^TX\\beta$\n",
    "\n",
    "Hence, $l(\\beta)=y^Ty-2y^TX\\beta+\\beta^TX^TX\\beta$\n",
    "\n",
    "#### Results: $\\frac{\\partial (a^T\\beta)}{\\partial\\beta}=a$ and $\\frac{\\partial (\\beta^TA\\beta)}{\\partial\\beta}=(A+A^T)\\beta=2A\\beta ~~~~~\\text{(if A is symmetric)}$\n",
    "\n",
    "Using these results $\\frac{\\partial ~l}{\\partial\\beta}=-2X^Ty+2X^TX\\beta=-2X^T(y-X\\beta)$\n",
    "\n",
    "Hence, $\\frac{\\partial ~l}{\\partial\\beta}=0 \\Rightarrow X^TX\\beta=X^Ty$\n",
    "\n",
    "$\\Rightarrow \\hat\\beta= (X^TX)^{-1}X^Ty$\n",
    "\n",
    "Also, $\\frac{\\partial^2 ~l}{\\partial\\beta^2}=2X^TX$ which is positive definite and hence the minimum exists at $\\hat\\beta$\n",
    "\n",
    "\n",
    "Our assumption is $Y\\sim \\text{Normal}(\\mu,\\sigma^2)$.\n",
    "\n",
    "    \n",
    "$\\therefore$ its pdf is $$ f(Y)=\\frac{1}{\\sigma\\,\\sqrt{2\\pi}}\\,e^{-\\frac{(Y-\\mu)^2}{2\\sigma^2}}$$ where $\\mu_i=\\sum_{j=0}^k \\beta_j\\,x_{ij}$ or in matrix notation, $\\mu= X\\beta$ \n",
    "    \n",
    "$\\Rightarrow$ the log likelihood (excluding constant) is $$l(\\beta|Y) = -\\frac{\\sum_i (y_i-\\mu_i)^2}{2\\sigma^2}$$\n",
    "    \n",
    "$$= \\sum_{i=1}^{n}(y_i-\\sum_{j=0}^{k}\\beta_j x_{ij})^2$$\n",
    "    \n",
    "Hence, to maximize $l(\\beta|Y)$, it is enough to minimize $\\frac{\\sum_i (y_i-\\mu_i)^2}{2\\sigma^2}$. This is because maximizing a function f(x) is equivalent to minimizing its negative, âˆ’f(x)\n",
    "\n",
    "In matrix notation, \n",
    "\n",
    "$l(\\beta)=||Y-X\\beta||^2=(Y-X\\beta)^T(Y-X\\beta)$\n",
    "\n",
    "=$(Y^T-\\beta^TX^T)(Y-X\\beta)$\n",
    "\n",
    "=$Y^TY-Y^TX\\beta-\\beta^TX^TY+\\beta^TX^TX\\beta$. It can be observed that $Y^TX\\beta$ is a scalar, A matrix of order $1\\times 1$. \n",
    "    \n",
    "Furthermore $\\beta^TX^TY=(Y^TX\\beta)^T$, and transpose of a scalar is the same scalar. This implies that, $-Y^TX\\beta-\\beta^TX^TY=-2Y^TX\\beta$\n",
    "\n",
    "Hence, $l(\\beta)=Y^TY-2Y^TX\\beta+\\beta^TX^TX\\beta$\n",
    "    \n",
    "Now let us use two results from matrix differentiation,\n",
    "    \n",
    "1. $\\frac{\\partial (a^T\\beta)}{\\partial\\beta}=a$ \n",
    "    \n",
    "1. $\\frac{\\partial (\\beta^TA\\beta)}{\\partial\\beta}=(A+A^T)\\beta=2A\\beta ~~~~~\\text{(if A is symmetric)}$\n",
    "    \n",
    "Using these results $\\frac{\\partial ~l}{\\partial\\beta}=-2X^TY+2X^TX\\beta$\n",
    "\n",
    "To minimize, we need to equate the first derivative to zero\n",
    "\n",
    "Hence, $\\frac{\\partial ~l}{\\partial\\beta}=0 \\Rightarrow X^TX\\beta=X^TY$\n",
    "\n",
    "$\\Rightarrow \\hat\\beta= (X^TX)^{-1}X^TY$ provided the inverse of the symmatrix matrix $(X^TX)_{p \\times p}$\n",
    "\n",
    "Also, $\\frac{\\partial^2 ~l}{\\partial\\beta^2}=2X^TX$ which is positive a definite matrix (a square matrix $A$ for which all its eigenvalues are positive, and columns of $A$ are linearly independent)\n",
    "\n",
    "\n",
    "Hence the minimum exists at $\\hat\\beta = (X^TX)^{-1}X^TY$\n",
    "    \n",
    "This gives a closed form solution for the linear model in the matrix form.\n",
    "    \n",
    "## <font color = darkgreen> Summary - OLS Procedure with Cost Function\n",
    "\n",
    "Given the linear regression model:\n",
    "\n",
    "$$Y_{n \\times 1} = X_{n \\times p} \\beta + \\epsilon_{n \\times 1}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $Y$ is the $n \\times 1$ vector of observed response values.\n",
    "- $X$ is the $n \\times p$ matrix of predictors (independent variables).\n",
    "- $\\beta$ is the $p \\times 1$ vector of unknown regression coefficients.\n",
    "- $\\epsilon$ is the $n \\times 1$ vector of errors (residuals).\n",
    "\n",
    "### <font color = darkblue> Objective: Minimize the Cost Function\n",
    "\n",
    "The goal of **Ordinary Least Squares (OLS)** is to estimate the coefficients $\\beta$ by minimizing the **cost function**, which is the sum of squared residuals (errors). The residuals are the differences between the observed values $Y$ and the predicted values $X\\beta$.\n",
    "\n",
    "The **cost function** $J(\\beta)$ is given by:\n",
    "\n",
    "$$J(\\beta) = \\| Y - X\\beta \\|^2$$\n",
    "\n",
    "This represents the sum of squared residuals, and we seek to minimize this function in order to find the best-fitting coefficients $\\beta$.\n",
    "\n",
    "### Solution\n",
    "\n",
    "To minimize the cost function, we take the derivative of $J(\\beta)$ with respect to $\\beta$, set it equal to zero, and solve for $\\beta$. The closed-form solution for the estimated coefficients is:\n",
    "\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T Y$$\n",
    "\n",
    "### <font color = darkblue> Assumptions\n",
    "\n",
    "- The matrix $X^T X$ must be **invertible** for the solution to exist. This requires that $X$ has **full column rank** (i.e., the predictors must be linearly independent).\n",
    "- If $X^T X$ is not invertible (e.g., if some columns of $X$ are linearly dependent), the solution does not exist, or the matrix may only be positive semi-definite.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <font color = darkblue> Key Points:\n",
    "1. The cost function is minimized to estimate the regression coefficients $\\beta$.\n",
    "\n",
    "2. The solution is derived by solving for $\\beta$ in the closed-form equation $\\hat{\\beta} = (X^T X)^{-1} X^T Y$.\n",
    "\n",
    "3. The matrix $X^T X$ must be invertible, which requires that the columns of $X$ are linearly independent.\n",
    "    \n",
    "4. OLS estimates the coefficients $\\beta$ by minimizing the **cost function** $J(\\beta) = \\| Y - X\\beta \\|^2$. The solution to this minimization problem is given by the closed-form equation:\n",
    "\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T Y$$\n",
    "\n",
    "This formula provides the best-fit linear regression coefficients, assuming that $X^T X$ is invertible and that $X$ has full column rank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = maroon> Implementation of OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5. 19.]\n",
      " [19. 87.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data points\n",
    "xd1 = np.array([1, 5, 6, 4, 3]).reshape(5, 1)\n",
    "\n",
    "# Number of data points, Number of regressors\n",
    "nd, ni = xd1.shape\n",
    "p = ni + 1\n",
    "xd = xd1\n",
    "\n",
    "# One's created as a column matrix\n",
    "xo = np.ones((nd, 1))\n",
    "\n",
    "# Coefficients matrix\n",
    "xm = xd\n",
    "\n",
    "# Modal matrix (X matrix with column of ones prepended)\n",
    "x = np.hstack((xo, xm))\n",
    "\n",
    "# X'X\n",
    "m = np.dot(x.T, x)\n",
    "\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det(model matrix): 74.0\n",
      "\n",
      "Matrix X'X is invertible\n",
      "[[ 1.17567568 -0.25675676]\n",
      " [-0.25675676  0.06756757]]\n",
      "\n",
      "Intercept (beta_0): [1.8514]\n",
      "\n",
      "Variable coefficients (beta_1): [[0.9865]]\n"
     ]
    }
   ],
   "source": [
    "# Determinant of m to check singularity\n",
    "d = np.linalg.det(m)\n",
    "\n",
    "print(\"det(model matrix):\",np.round(d,4))\n",
    "print()\n",
    "\n",
    "if d == 0:\n",
    "    print(\"Matrix X'X is singular\")\n",
    "else:\n",
    "    print(\"Matrix X'X is invertible\")\n",
    "\n",
    "# Response variable\n",
    "y = np.array([4, 6, 10, 3, 5]).reshape(5, 1)\n",
    "\n",
    "# OLS - Ordinary Least Squares\n",
    "if d != 0:\n",
    "    mi = np.linalg.inv(m)    # Inverse of X'X\n",
    "        \n",
    "print(mi)\n",
    "print()\n",
    "\n",
    "# Regression beta coefficients\n",
    "betas = np.dot(mi, np.dot(x.T, y))\n",
    "intercept = np.round(betas[0],4)  # The first value is the intercept (beta_0)\n",
    "beta_values = np.round(betas[1:] ,4) # The second value is beta_1\n",
    "\n",
    "print(\"Intercept (beta_0):\", intercept)\n",
    "print()\n",
    "print(\"Variable coefficients (beta_1):\", beta_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient (Slope): [[0.98648649]]\n",
      "Intercept: [1.85135135]\n",
      "Predicted values: [[2.83783784]\n",
      " [6.78378378]\n",
      " [7.77027027]\n",
      " [5.7972973 ]\n",
      " [4.81081081]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data points\n",
    "xd1 = np.array([1, 5, 6, 4, 3]).reshape(5, 1)  # Independent variable\n",
    "# Response variable\n",
    "y = np.array([4, 6, 10, 3, 5]).reshape(5, 1)  # Dependent variable\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(xd1, y)\n",
    "\n",
    "# Print the coefficient (slope) and intercept\n",
    "print(f\"Coefficient (Slope): {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "\n",
    "# Predict values based on the fitted model\n",
    "y_pred = model.predict(xd1)\n",
    "\n",
    "print(\"Predicted values:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue> Gradient Descent (GD) Algorithm for Linear Model Estimation\n",
    "\n",
    "### <font color = green> What is Gradient Descent?\n",
    "\n",
    "**Gradient Descent (GD)** is an iterative optimization algorithm used to minimize a cost function. It is commonly used when the closed-form solution to the optimization problem is difficult or computationally expensive to obtain.\n",
    "\n",
    "In the context of **Linear Models (LM)**, we typically use Gradient Descent to find the optimal parameters (or coefficients) $\\beta$ that minimize the **cost function**, which is often the **sum of squared residuals** \n",
    "\n",
    "Given the linear model:\n",
    "\n",
    "$$\n",
    "Y_{n \\times 1} = X_{n \\times p} \\beta + \\epsilon_{n \\times 1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $Y$ is the $n \\times 1$ vector of observed response values.\n",
    "- $X$ is the $n \\times p$ matrix of predictors (independent variables).\n",
    "- $\\beta$ is the $p \\times 1$ vector of regression coefficients (the parameters to be estimated).\n",
    "- $\\epsilon$ is the $n \\times 1$ vector of errors (residuals).\n",
    "\n",
    "We aim to estimate $\\beta$ by minimizing the **cost function**:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\| Y - X\\beta \\|^2\n",
    "$$\n",
    "\n",
    "### <font color = green> How Gradient Descent Works\n",
    "\n",
    "The idea behind Gradient Descent is to iteratively adjust the parameters $\\beta$ in the direction that reduces the value of the cost function. The algorithm works by computing the **gradient** (or partial derivatives) of the cost function with respect to $\\beta$, and then updating $\\beta$ in the opposite direction of the gradient.\n",
    "\n",
    "The **update rule** for Gradient Descent is:\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\nabla_{\\beta} J(\\beta^{(t)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\beta^{(t)}$ is the vector of parameters at iteration $t$.\n",
    "- $\\alpha$ is the **learning rate**, a positive scalar that controls the step size of each update.\n",
    "- $\\nabla_{\\beta} J(\\beta^{(t)})$ is the gradient of the cost function $J(\\beta)$ with respect to $\\beta$ at iteration $t$.\n",
    "\n",
    "The gradient of the cost function with respect to $\\beta$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} J(\\beta) = -2 X^T (Y - X\\beta)\n",
    "$$\n",
    "\n",
    "Thus, the update rule becomes:\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} + 2\\alpha X^T (Y - X\\beta^{(t)})\n",
    "$$\n",
    "\n",
    "### <font color = green> Gradient Descent for Linear Model Estimation\n",
    "\n",
    "1. **Initialization**: Start with an initial guess for $\\beta^{(0)}$. This can be a vector of zeros or some other small random values.\n",
    "\n",
    "2. **Iterative Updates**: At each iteration, update the value of $\\beta$ using the gradient of the cost function. The update depends on the current value of $\\beta$ and the **learning rate** $\\alpha$.\n",
    "\n",
    "3. **Convergence**: The algorithm continues iterating until the change in the cost function $J(\\beta)$ is sufficiently small, indicating convergence. This means that the algorithm has found a solution where the cost function is near its minimum.\n",
    "\n",
    "### <font color = green> Advantages of Gradient Descent in Linear Model Estimation\n",
    "\n",
    "1. **Scalability**: Gradient Descent is particularly useful when dealing with large datasets, as it does not require the computation of the inverse of $X^T X$, which can be computationally expensive for large $n$ and $p$. Unlike the closed-form OLS solution, which requires $(X^T X)^{-1}$, Gradient Descent is an iterative process and can be applied to problems with many features or large datasets.\n",
    "\n",
    "2. **Flexibility**: It can be used with different cost functions and regularization techniques (e.g., **Ridge Regression** or **Lasso**) where the closed-form solution might not exist.\n",
    "\n",
    "3. **Handling Non-linearity**: While OLS assumes a linear model, Gradient Descent can be extended to more complex models that may not have closed-form solutions, such as neural networks and other machine learning algorithms.\n",
    "\n",
    "### <font color = green> Key Considerations for Gradient Descent\n",
    "\n",
    "- **Learning Rate**: The choice of the learning rate $\\alpha$ is crucial. If $\\alpha$ is too large, the algorithm may overshoot the optimal solution and fail to converge. If $\\alpha$ is too small, the algorithm may converge very slowly. Therefore, selecting an appropriate learning rate is important.\n",
    "  \n",
    "- **Convergence**: In practice, Gradient Descent may converge to a local minimum or require many iterations to converge, especially in high-dimensional settings. Techniques like **stochastic gradient descent** (SGD) and **mini-batch gradient descent** can help in speeding up convergence in large datasets.\n",
    "\n",
    "### <font color = green> Summary\n",
    "\n",
    "- **Gradient Descent (GD)** is an optimization algorithm used to find the optimal coefficients $\\beta$ in a linear regression model.\n",
    "\n",
    "- It iteratively updates $\\beta$ by computing the gradient of the cost function with respect to $\\beta$ and adjusting the parameters in the direction that minimizes the cost.\n",
    "\n",
    "- GD is useful in scenarios where the closed-form solution for linear regression (OLS) is computationally expensive or infeasible, especially with large datasets or high-dimensional problems.\n",
    "\n",
    "\n",
    "### <font color = green> Key Points:\n",
    "\n",
    "- **Gradient Descent** is used to minimize the **cost function** $ J(\\beta) = \\| Y - X\\beta \\|^2 $ iteratively.\n",
    "\n",
    "- **Iterative Updates**: The parameters $ \\beta $ are updated by moving in the direction opposite to the gradient of the cost function.\n",
    "\n",
    "- **Advantages**: GD is particularly helpful for large datasets or models that are difficult to solve with a closed-form solution (like OLS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue> Comparison of **OLS (Ordinary Least Squares)** and **Gradient Descent (GD)**\n",
    "\n",
    "Both **OLS** and **Gradient Descent (GD)** are used to estimate the coefficients ($\\beta$) in linear regression models, but they approach the problem in different ways. Below is a detailed comparison:\n",
    "\n",
    "\n",
    "\n",
    "### <font color = green> 1. **Methodology**\n",
    "\n",
    "- **OLS (Ordinary Least Squares)**:\n",
    "  - **Closed-form solution**: OLS provides a direct, closed-form solution for the coefficients $\\beta$.\n",
    "  - The formula for the coefficients is:\n",
    "    $\\hat{\\beta} = (X^T X)^{-1} X^T Y$\n",
    "  - The solution is derived by minimizing the sum of squared residuals  using matrix algebra.\n",
    "  \n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Iterative optimization**: GD is an iterative algorithm that estimates the coefficients $\\beta$ by continuously adjusting them to minimize the cost function (sum of squared residuals).\n",
    "  - The update rule for GD is:\n",
    "    $\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\nabla_{\\beta} J(\\beta^{(t)})$\n",
    "  - It uses the gradient of the cost function to iteratively move towards the optimal solution.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 2. **Computation**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Direct Calculation**: OLS directly calculates $\\hat{\\beta}$ in one step by solving the matrix equation. This involves computing $ (X^T X)^{-1} $, which can be computationally expensive, especially for large datasets.\n",
    "  - Requires inversion of the matrix $X^T X$, which can be problematic if $X^T X$ is singular (non-invertible).\n",
    "\n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Iterative Calculation**: GD doesn't require matrix inversion. Instead, it works iteratively, updating the coefficients in small steps based on the gradient of the cost function.\n",
    "  - It can work with large datasets and high-dimensional features since it doesn't involve matrix inversion.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 3. **Convergence and Speed**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Faster for small to medium datasets**: OLS is efficient for small to medium-sized datasets, as the closed-form solution is quick to compute.\n",
    "  - **No iterations**: Since OLS provides a direct solution, it does not require iterations or hyperparameter tuning.\n",
    "  \n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Slower and iterative**: GD can be slower, as it requires multiple iterations to converge to the optimal solution. The speed of convergence depends on factors such as the learning rate ($\\alpha$) and the number of iterations.\n",
    "  - **Can be computationally expensive for large datasets**: Each iteration involves computing the gradient, which can be costly for very large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 4. **Scalability**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Limited scalability**: OLS can be inefficient for very large datasets, especially when the matrix $X^T X$ is large and inversion becomes computationally expensive.\n",
    "  - May not work well with high-dimensional data (large $p$) or when $X^T X$ is singular or nearly singular.\n",
    "\n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Highly scalable**: GD is more scalable because it does not require matrix inversion. It can be applied to large datasets and high-dimensional problems.\n",
    "  - Can also be applied to **mini-batch gradient descent** or **stochastic gradient descent (SGD)**, which can speed up the convergence by processing small subsets (batches) of data.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 5. **Requirements and Assumptions**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Assumes full rank**: OLS assumes that the matrix $X^T X$ is invertible, which requires $X$ to have full column rank (i.e., the columns of $X$ should be linearly independent).\n",
    "  - Works best when the matrix is not too large or singular.\n",
    "  \n",
    "- **Gradient Descent (GD)**:\n",
    "  - **No assumption of invertibility**: GD does not require the matrix $X^T X$ to be invertible. It works iteratively and can handle situations where OLS may fail (e.g., with singular matrices).\n",
    "  - However, it does require selecting an appropriate **learning rate** ($\\alpha$) to ensure convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 6. **Handling Large and Complex Data**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Not ideal for large datasets**: OLS is not ideal for very large datasets because the matrix inversion $ (X^T X)^{-1} $ becomes computationally expensive and memory-intensive.\n",
    "  - May struggle with high-dimensional data when $p$ (number of predictors) is large relative to $n$ (number of observations).\n",
    "\n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Ideal for large datasets**: GD is well-suited for large datasets and high-dimensional problems because it processes data in small batches (in the case of stochastic gradient descent).\n",
    "  - Can be applied to datasets where the number of features $p$ is much larger than the number of observations $n$.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 7. **Precision and Accuracy**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Exact solution**: OLS provides an exact solution (assuming the conditions for invertibility hold). It guarantees that the coefficients found are the best fit for minimizing the sum of squared residuals, as long as the matrix $X^T X$ is invertible.\n",
    "\n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Approximate solution**: GD provides an approximate solution, and the precision depends on the number of iterations and the learning rate $\\alpha$.\n",
    "  - May converge to a local minimum or take longer to converge, depending on the starting point and the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 8. **Robustness**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Sensitive to outliers**: OLS is sensitive to outliers because it minimizes the sum of squared residuals, which can be disproportionately influenced by large residuals.\n",
    "  \n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Can be made more robust**: GD can be made more robust by using regularization techniques (like Ridge or Lasso) or by applying more sophisticated variants like **stochastic gradient descent (SGD)** with learning rate schedules.\n",
    "\n",
    "### <font color = green> 9. **Algorithmic Complexity**\n",
    "\n",
    "- **OLS (Ordinary Least Squares)**:\n",
    "  - The closed-form solution of OLS involves matrix operations like matrix multiplication and inversion. The time complexity for calculating $ \\hat{\\beta} = (X^T X)^{-1} X^T Y $ is dominated by the matrix inversion step, which is $ O(p^3) $, where $ p $ is the number of predictors (features).\n",
    "  - The complexity of matrix multiplication $ X^T X $ is $ O(np^2) $, and the inversion of $ X^T X $ takes $ O(p^3) $.\n",
    "  - **Total Time Complexity**: $ O(np^2 + p^3) $.\n",
    "\n",
    "- **Gradient Descent (GD)**:\n",
    "  \n",
    "    - In each iteration, the time complexity of computing the gradient $ \\nabla_{\\beta} J(\\beta) = -2 X^T (Y - X\\beta) $ is $ O(np) $, as it involves multiplying the $ n \\times p $ matrix $ X^T $ with the residual vector $ Y - X\\beta $.\n",
    "  \n",
    "    - The number of iterations required for convergence depends on factors like the learning rate and convergence criteria. If we assume $ T $ iterations to reach convergence, the time complexity of GD is $ O(T \\cdot np) $.\n",
    "  \n",
    "    - **Total Time Complexity**: $ O(Tnp) $, where $ T $ is the number of iterations.\n",
    "\n",
    "\n",
    "\n",
    "### Comparison Table \n",
    "\n",
    "| Feature                  | OLS (Ordinary Least Squares)                  | Gradient Descent (GD)                           |\n",
    "|--------------------------|-----------------------------------------------|-------------------------------------------------|\n",
    "| **Solution Type**         | Closed-form solution                          | Iterative solution                              |\n",
    "| **Computation**           | Requires matrix inversion                     | Iterative updates (gradient computation)        |\n",
    "| **Speed**                 | Faster for small datasets, no iterations      | Slower, but can be controlled by learning rate |\n",
    "| **Scalability**           | Limited to small/medium datasets              | Highly scalable, works for large datasets       |\n",
    "| **Assumptions**           | Full rank of $ X^T X $, invertibility       | No assumptions about matrix invertibility       |\n",
    "| **Handling Large Data**   | Inefficient for very large data               | Efficient, handles large datasets well          |\n",
    "| **Precision**             | Exact solution (if conditions hold)           | Approximate solution, depends on iteration      |\n",
    "| **Algorithmic Complexity**| $ O(np^2 + p^3) $                           | $ O(Tnp) $, where $ T $ is the number of iterations |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "- **OLS** is fast and provides an exact solution when the design matrix $X$ is not too large and is invertible, making it ideal for small to medium-sized datasets. It is computationally expensive when the number of features $ k $is large or when the dataset $ n $ is large, due to the $ O(p^3) $ complexity from matrix inversion.\n",
    "\n",
    "- **Gradient Descent** is more scalable and suitable for large datasets or high-dimensional problems. The time complexity depends on the number of iterations, but the per-iteration cost is generally $ O(np)$, which is more efficient than OLS for large-scale problems. However, the convergence of GD can be slower, and the number of iterations needed depends on factors like the learning rate and data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:15px\">\n",
    "\n",
    "# <font color = maroon> Genearlized Linear Model (GLM) \n",
    "    \n",
    "GLMs extend linear models by allowing the dependent variable to follow different distributions (binomial, Poisson, etc.) and using a link function to relate the predictors to the expected value of the dependent variable. \n",
    "    \n",
    "    \n",
    "**Distribution of the Dependent Variable**\n",
    "\n",
    "* Linear Models: Assume that the dependent variable is continuous and follows a normal distribution. This means the outcomes are expected to be spread out symmetrically around a mean.\n",
    "    \n",
    "\n",
    "* GLMs: Allow the dependent variable to follow any distribution from the exponential family (e.g., binomial, Poisson, gamma). This flexibility enables GLMs to model different types of data, such as counts (Poisson), probabilities (binomial), or skewed distributions (gamma).\n",
    "    \n",
    "**Type of Outcome**\n",
    "\n",
    "* Linear Models: Used when the dependent variable is continuous, for example, predicting quantities like temperature, sales, or height. The model assumes the relationship between predictors and the outcome is linear.\n",
    "    \n",
    "\n",
    "* GLMs: Can be used for a broader range of data types, including binary outcomes (yes/no, 0/1), count data (e.g., number of events), and other non-continuous data types. For example, GLMs are used in logistic regression (for binary outcomes) or Poisson regression (for count data).\n",
    "\n",
    "<hr style=\"border: 1.5px solid blue\"/> \n",
    "\n",
    "## <font color = darkgreen> Three Components of GLM \n",
    "\n",
    "1. <font color = red> **Random Component:** \n",
    "    \n",
    "    Response variable $Y$ with independent observations $(y_1,., y_n)$ having probability density (or mass) function of a distribution\n",
    "    \n",
    "\n",
    "2. <font color = blue>**Linear Predictor:** \n",
    "    \n",
    "    Consider a set of $k$ predictors or explanatory variables $X_1, X_2, \\cdots \\cdots , X_k$. Also we set $X_0$, as the intercept or constant term in the model so that there will be $p = k+1$ predictors, $X_0,X_1, X_2, \\cdots \\cdots , X_k$. \n",
    "    \n",
    "    Let $x_{ij}$ denote the value of explanatory variable  $X_{j}$ corresponding to $i^{\\text{th}}$ observation where $i=1,2,3, \\cdots \\cdots , n$ and $j=0,1,2, \\cdots \\cdots , k$. It can be noted that $x_{i0}=1$ for all $i=1,2,3, \\cdots \\cdots , n$\n",
    "    \n",
    "     \n",
    "3. <font color = purple>**Link Function:** \n",
    "    \n",
    "    A relation between the mean of the response variable $\\mu_i=E(Y_i)$ and the explaintory variables $X_0$,$X_1$,$X_2$,....,$X_k$ through the linear combination $\\sum\\beta_jx_{ij},~~~~~~j=0,1,2,\\cdots \\cdots ,k$ and $i = 1,2,\\cdots \\cdots , n$. That is $\\eta_i=g(\\mu_i)=\\sum\\beta_jx_{ij}$\n",
    "\n",
    "<hr style=\"border: 1.5px solid blue\"/>    \n",
    "\n",
    "\n",
    "## Necessary Symbols:\n",
    "\n",
    "- k: Number of predictors\n",
    "    \n",
    "- p = k + 1 (number of predictors + constant term)\n",
    "    \n",
    "- X: The model matrix of predictors, including the constant term, is represented as:  \n",
    "$$X = \\begin{pmatrix}\n",
    "  x_{ij} \\end{pmatrix}_{n\\times p}$$  \n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "  x_{10} & x_{11} & \\cdots & x_{1k} \\\\\n",
    "  x_{20} & x_{21} & \\cdots & x_{2k} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  x_{n0} & x_{n1} & \\cdots & x_{nk}\n",
    "\\end{pmatrix}_{n \\times p}\n",
    "$$ \n",
    "\n",
    "where $x_{i0} = 1 ~~\\forall i = 1,2\\cdots n$, representing the intercept term (constant term) in the linear model and $x_{ij} $ for $j = 1, 2, 3, \\dots, k$ are the values of the predictor variables for each of the $n$ data points.\n",
    "\n",
    "- Y: Response variable \n",
    "$$Y =\n",
    " \\begin{pmatrix}\n",
    "  y_{1} & y_{2} & \\cdots & y_{n} \n",
    " \\end{pmatrix}^T_{n\\times 1}$$\n",
    "\n",
    "    \n",
    "- Mean Response $\\mu = E[Y]$\n",
    "$$\\mu =\n",
    " \\begin{pmatrix}\n",
    "  \\mu_{1} & \\mu_{2} & \\cdots & \\mu_{n} \n",
    " \\end{pmatrix}^T_{n\\times 1}$$ with $\\mu_i=E[y_i]$\n",
    " \n",
    "    \n",
    "- $\\beta$: Parameter matrix \n",
    " $$\\beta =\n",
    " \\begin{pmatrix}\n",
    "  \\beta_{1} & \\beta_{2} &   \\cdots  & \\beta_{p} \n",
    " \\end{pmatrix}^T_{p\\times 1}$$ \n",
    " \n",
    "    \n",
    "- $\\eta$: Link function, a function $g(\\mu)$ of mean of the response variable.\n",
    " $$\\eta =\n",
    " \\begin{pmatrix}\n",
    "  \\eta_{1} &  \\eta_{2} &  \\cdots & \\eta_{n} \n",
    " \\end{pmatrix}^T_{n\\times 1}$$ with $\\eta_i=g(\\mu_i)=\\sum_{i=1}^n\\beta_jx_{ij}$\n",
    "\n",
    "<hr style=\"border: 1.5px solid blue\"/> \n",
    "\n",
    "## <font color = darkgreen> Estimation - Fitting GLM\n",
    "\n",
    "First let us consider the exponential family of distribution that has likelihood of the form\n",
    "\n",
    "$f(y|\\theta,\\phi) = \\exp(\\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y,\\phi))$ where $\\theta$ is natural parameter and $\\phi$ is scale parameter\n",
    "    \n",
    "The log likelihood is\n",
    "\n",
    "$l =  \\sum^{n}_{i=1} (\\frac{y_{i}~\\theta_{i} - b(\\theta_{i})}{a(\\phi_{i})} + c(y_{i},\\phi_{i}))$\n",
    "\n",
    "Then the $i^{th}$ component of the log likelihood is \n",
    "$l_i =  \\frac{y_i~\\theta_{i} - b(\\theta_{i})}{a(\\phi_{i})} + c(y_{i},\\phi_{i}) \\cdots \\cdots (1)$\n",
    "\n",
    "The aim is to maximise $l_i$ with respect to the parameter $\\beta_i$. This can be done using the relationship between the log likelihood $l$ and the parameter $\\beta$ through $\\theta$, the mean of the random component $\\mu=E[Y]$ and the link function $\\eta=g(\\mu)$, a function of mean of $Y$\n",
    "    \n",
    "That is, $l_i \\, \\rightarrow \\theta_i  \\, \\rightarrow \\mu_i  \\, \\rightarrow \\eta_i  \\, \\rightarrow \\beta_j$\n",
    "    \n",
    "Then from the chain rule, \n",
    "\n",
    "$$\\frac{\\partial l_{i}}{\\partial \\beta_{j}}   = \\frac{\\partial l_{i}}{\\partial \\theta_{i}}  \\frac{\\partial \\theta_{i}}{\\partial \\mu_{i}} \\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}\\frac{\\partial \\eta_{i}}{\\partial \\beta_{j}} \\cdots \\cdots \\cdots (2)$$\n",
    "    \n",
    "Before proceeding further, let us consider some preliminaries in breaking down the quantities in the equation $(2)$ \n",
    "\n",
    "### <font color = darkblue> Result 1:$E[\\frac{\\partial l}{\\partial \\theta }] = 0$\n",
    "\n",
    "### <font color = darkblue> Result 2:$E[\\frac{\\partial^{2} l}{\\partial \\theta^{2} }] = -E[(\\frac{\\partial l}{\\partial \\theta })^{2}]$\n",
    "\n",
    "From $(1)$, $$\\frac{\\partial l}{\\partial \\theta} = \\frac{y-b'(\\theta)}{a(\\phi)} \\cdots \\cdots (3)$$ \n",
    "    \n",
    "$$\\frac{\\partial^{2} l}{\\partial \\theta^{2}}=-\\frac{b^{''}(\\theta)}{a(\\phi)}  \\cdots \\cdots (4)$$\n",
    "\n",
    "Applying Result 1 in (3) will imply that $E[\\frac{y-b'(\\theta)}{a(\\phi)}]=0$ \n",
    "\n",
    "$\\implies E[y-b'(\\theta)]=0$ or,\n",
    "\n",
    "$$E[y] = b'(\\theta)\\cdots \\cdots (5)$$   \n",
    "\n",
    "Now consider $E[(\\frac{\\partial l}{\\partial \\theta})^2]$\n",
    "    \n",
    "$$E[(\\frac{\\partial l}{\\partial \\theta})^2]= E[(\\frac{y-b'(\\theta)}{a(\\phi)})^2]$$\n",
    "    \n",
    "$$=\\frac{1}{a(\\phi)^2}E[(y-b'(\\theta))^2]$$\n",
    "    \n",
    "$$=\\frac{1}{a(\\phi)^2}E[(y-E(y))^2] \\,\\,\\text{(using (5))}$$\n",
    "    \n",
    "$$=\\frac{1}{a(\\phi)^2}V(y)\\,\\,\\text{(using the definition of variance of a random variable)}$$\n",
    "\n",
    "An immediate consequence of Result 2 and (4) is, $$ -\\frac{b^{''}(\\theta)}{a(\\phi)}=-\\frac{1}{a(\\phi)^2}V(y)$$ \n",
    "    \n",
    "$$\\implies V(Y) = a(\\phi)b^{''}(\\theta) \\cdots \\cdots (6)$$\n",
    "\n",
    "Equations (5) and (6) provide the formula for finding the moments of $y$ in terms of $b(\\theta)$ and $a(\\phi)$\n",
    "\n",
    "Let us consider the terms in (2)\n",
    "\n",
    "From (3), $\\frac{\\partial l_i}{\\partial \\theta_i} =  \\frac{y_i-b'(\\theta_i)}{a(\\phi)}$\n",
    "\n",
    "From (5), $E[y_i] = \\mu_i = b'(\\theta_i)$,\n",
    "\n",
    "$\\implies \\frac{\\partial \\mu_i}{\\partial \\theta_i}=b{''}(\\theta_i)$ \n",
    "\n",
    "$\\implies \\frac{\\partial \\theta_i }{\\partial \\mu_i}=\\frac{1}{b{''}(\\theta_i)}$ \n",
    "\n",
    "Also the link function $(\\eta_i)$ is related to the linear predicator, $\\eta_i = \\sum x_{ij}\\beta_j$\n",
    "\n",
    "$\\implies \\frac{\\partial \\eta_i}{\\partial \\beta_j}=x_{ij}$\n",
    "\n",
    "Hence (2) becomes\n",
    "\n",
    "$\\frac{\\partial l_i}{\\partial \\beta_j}=\\frac{y_i-b'(\\theta_i)}{a(\\phi)}\\frac{1}{b{''}(\\theta_i)} \\frac{\\partial \\mu_i}{\\partial \\eta_i}x_{ij}$\n",
    "\n",
    "But (6), $b^{''}(\\theta_i)=\\frac{V(y_i)}{a(\\phi)}$\n",
    "\n",
    "$\\implies \\frac{1}{b^{''}(\\theta_i)}=\\frac{a(\\phi)}{V(y_i)}$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$\\frac{\\partial l_i}{\\partial \\beta_j}=\\frac{y_i-b'(\\theta_i)}{V(y_i)}\\frac{\\partial \\mu_i}{\\partial \\eta_i}x_{ij}$\n",
    "\n",
    "Also (5) can be used to get,\n",
    "\n",
    "$$\\frac{\\partial l_i}{\\partial \\beta_j}=\\frac{y_i-\\mu_i}{V(y_i)}\\frac{\\partial \\mu_i}{\\partial \\eta_i}x_{ij}$$\n",
    "\n",
    "$$\\implies \\frac{\\partial l}{\\partial \\beta}=\\sum_{i=1}^n\\frac{y_i-\\mu_i}{V(y_i)}\\frac{\\partial \\mu_i}{\\partial \\eta_i}x_{ij}$$\n",
    "    \n",
    "This is the $j^{th}$ element of the $p \\times 1$ matrix $\\frac{\\partial l}{\\partial \\beta}$ \n",
    "\n",
    "<hr style=\"border: 1.5px solid blue\"/>  \n",
    "    \n",
    "Let us define two diagonal matrices $D$ and $V$ of order $n \\times n$ where,\n",
    "\n",
    "$D=\\text{Diag} (\\frac{\\partial \\mu_i}{\\partial \\eta_i}) = \\begin{pmatrix}\n",
    "  \\frac{\\partial \\mu_1}{\\partial \\eta_1} & 0 & \\cdots & 0 \\\\\n",
    "  0 & \\frac{\\partial \\mu_2}{\\partial \\eta_2} & \\cdots & 0 \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  0 & 0 & \\cdots & \\frac{\\partial \\mu_n}{\\partial \\eta_n}\n",
    "\\end{pmatrix}_{n \\times n}$\n",
    "\n",
    "$V=\\text{Diag} (V(y_i))= \\begin{pmatrix}\n",
    "  V(Y_1) & 0 & \\cdots & 0 \\\\\n",
    "  0 & V(Y_2) & \\cdots & 0 \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  0 & 0 & \\cdots & V(Y_n)\n",
    "\\end{pmatrix}_{n \\times n}$\n",
    "\n",
    "$\\implies V^{-1}=\\begin{pmatrix}\n",
    "  \\frac{1}{V(Y_1)} & 0 & \\cdots & 0 \\\\\n",
    "  0 & \\frac{1}{V(Y_2)} & \\cdots & 0 \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  0 & 0 & \\cdots & \\frac{1}{V(Y_n)}\n",
    "\\end{pmatrix}_{n \\times n}$\n",
    "\n",
    "Also,\n",
    "\n",
    "$Y-\\mu=\\begin{pmatrix}\n",
    "  y_{1}-\\mu_1 & y_{2}-\\mu_2 & \\cdots & y_{n}-\\mu_n \n",
    " \\end{pmatrix}^T_{n\\times 1}$\n",
    "\n",
    "\n",
    "Hence the $p \\times 1 $ matrix $\\frac{\\partial l}{\\partial \\beta}$ can be written in the matrix form using the matrices $X, V, D,$ and $Y-\\mu$. That is \n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\beta} = X^T  D V^{-1} (Y-\\mu) \\cdots \\cdots (7)$$\n",
    "\n",
    "<hr style=\"border: 1.5px solid blue\"/> \n",
    "\n",
    "\n",
    "Now from Result 2, $E[\\frac{\\partial^{2} l_{i}}{\\partial \\beta_{h}\\partial \\beta_{j}}]=-E[\\frac{\\partial l_i}{\\partial \\beta_h}\\frac{\\partial l_i}{\\partial \\beta_j}]$ \n",
    "\n",
    "This implies,\n",
    "    \n",
    "$E[-\\frac{\\partial^{2} l_{i}}{\\partial \\beta_{h}\\partial \\beta_{j}}]=~E[\\frac{y_{i} - \\mu_{i}}{V(y_{i})}\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}x_{ih} \\frac{y_{i} - \\mu_{i}}{V(y_{i})}\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}x_{ij} ]$\n",
    "\n",
    "$=~E\\large[\\large[\\frac{y_{i} - \\mu_{i}}{V(y_{i})}\\large]\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}x_{ih} \\frac{y_{i} - \\mu_{i}}{V(y_{i})}\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}x_{ij} \\large]$\n",
    "\n",
    "$=~E\\large[\\large(\\frac{y_{i} - \\mu_{i}}{V(y_{i})}\\large)^2\\large(\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}\\large)^2x_{ih} x_{ij} \\large]$\n",
    "\n",
    "$=\\frac{1}{V(y_{i})^2}E\\large[(y_{i} - \\mu_{i})^2\\large]\\large(\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}\\large)^2x_{ih} x_{ij}$ $\\,\\,\\,(\\because E(aX)=aE(X), \\text{where a is a constant})$\n",
    "\n",
    "$=\\frac{1}{V(y_{i})^2}V(y_{i})\\large(\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}\\large)^2x_{ih} x_{ij}$ $\\,\\,\\,(\\because \\mu_i=E(y_i) \\,\\,\\& \\,\\,V(X)=E[(X-E(X))^2])$\n",
    "\n",
    "$= \\frac{1}{V(y_{i})} \\large(\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}\\large)^2 \\,x_{ih}x_{ij}$\n",
    "\n",
    "$\\therefore -E\\large[\\frac{\\partial^{2} l}{\\partial \\beta_{h}\\partial \\beta_{j} }\\large] = \\sum^{n}_{i=1} \\frac{1}{V(Y_{i})} (\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i} })^{2}x_{ih}x_{ij} \\cdots \\cdots (8)$\n",
    "\n",
    "Let us define a diagonal matrix $W$ of order $n \\times n$ where,\n",
    "\n",
    "$W=\\text{Diag} \\frac{(\\frac{\\partial \\mu}{\\partial \\eta})^2}{V(y)} = \\begin{pmatrix}\n",
    "  \\frac{(\\frac{\\partial \\mu_1}{\\partial \\eta_1})^2}{V(y_1)} & 0 & \\cdots & 0 \\\\\n",
    "  0 &\\frac{(\\frac{\\partial \\mu_2}{\\partial \\eta_2})^2}{V(y_2)} & \\cdots & 0 \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  0 & 0 & \\cdots & \\frac{(\\frac{\\partial \\mu_n}{\\partial \\eta_n})^2}{V(y_n)}\n",
    "\\end{pmatrix}_{n \\times n}$\n",
    "\n",
    "Then the term inside the summation of (8) is the $(i,j)^{th}$ element of the matrix $X^T W X$ of order $p \\times p$\n",
    "    \n",
    "Hence, from (8) we have $$ -E\\large[\\frac{\\partial^{2} l}{\\partial \\beta_{h}\\partial \\beta_{j} }\\large] =X^T W X $$\n",
    "    \n",
    "Equivalently,\n",
    "    \n",
    "$$ -E\\large[H] =X^T W X \\cdots \\cdots (9)$$ where $H$ is the Hessain matrix, a matrix of second derivatives   \n",
    "\n",
    "<hr style=\"border: 1.5px solid blue\"/> \n",
    "    \n",
    "### <font color = darkgreen> Iterative Procedures\n",
    "    \n",
    "In maximizing the log likelihood, we apply Newton Raphson method \n",
    "    \n",
    "$$\\beta^{(t+1)}=\\beta^{(t)}-[H^{(t)}]^{-1} U^{(t)} \\cdots \\cdots (10)$$ where $H$ is the Hessain matrix and $U$ is the score function or gradient of $l$, the log likelihood \n",
    "    \n",
    "Also we shall replace $-H^{(t)}$ by the Fisher Information matrix $I_F = -E[H]$ so that (10) becomes,\n",
    "\n",
    "    \n",
    "$$\\beta^{(t+1)}=\\beta^{(t)}+[I_F^{(t)}]^{-1} U^{(t)}$$\n",
    "    \n",
    "The equations $(7)$ and $(9)$ provide the score funtion $U$ and Fisher Information $I_F$ respectively \n",
    "\n",
    "That is, \n",
    "    \n",
    "$$U = \\frac{\\partial l}{\\partial \\beta} = X^T  D V^{-1} (Y-\\mu)$$\n",
    "    \n",
    "    \n",
    "$$I_F =  -E\\large[H] =X^T W X $$ where \n",
    "    \n",
    "$D=\\text{Diag} (\\frac{\\partial \\mu_i}{\\partial \\eta_i})$\n",
    "    \n",
    "$V=\\text{Diag} (V(y_i))$\n",
    "    \n",
    "$W=\\text{Diag} \\frac{(\\frac{\\partial \\mu}{\\partial \\eta})^2}{V(y)}$\n",
    "\n",
    "<hr style=\"border: 1.5px solid blue\"/>     \n",
    "   \n",
    "    \n",
    "### <font color = steelblue>A note on Observed Information and Expected Information (Fisher Information)\n",
    "\n",
    "The **observed information** is defined as the negative of the second derivative of the log-likelihood function with respect to the parameter $ \\theta $. It quantifies the amount of information that the data provide about the parameters.\n",
    "\n",
    "$$\n",
    "I_{obs}(\\theta) = - H\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ l(\\theta) $ is the log-likelihood function.\n",
    "- $ H $ is the Hessian matrix (second derivative) of the log-likelihood.\n",
    "\n",
    "The **expected information** (also known as the **Fisher information**) is the **expectation** of the observed information. It represents the amount of information about the parameter $ \\theta $ averaged over all possible samples.\n",
    "\n",
    "$$\n",
    "I_F(\\theta) = -E[H]\n",
    "$$\n",
    "\n",
    "The Fisher information is a key concept in understanding the asymptotic properties of maximum likelihood estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = darkgreen> Implementation of Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 211 iterations\n",
      "Final Estimated Beta values: [[-2.52405709]\n",
      " [ 1.03169462]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAGDCAYAAABwRoerAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABBfElEQVR4nO3deXxU9b3/8deHECECgktBWVpQKUpBg8UF0RpKFVFEtLVKW5faFtdW/CkV9Lq19kovaq1Vr0uvt9QdN6SWClqJ3LpLoYgiFRWRgAhikAjBLJ/fH3MGJ8lMMklmPfN+Ph48MnPm5JzvzJeQN9/V3B0RERERyS0dsl0AEREREWlKIU1EREQkBymkiYiIiOQghTQRERGRHKSQJiIiIpKDFNJEREREcpBCmojkBDMrM7M12S6HJGZmd5jZldkuh0ihUEgTCRkzW2Vm28ysysw+NbO/mlm/JL+3zUHJzDqbWaWZfTvOa78zs0fbct1MMLNrzMzN7JBslyWRoIz3xTx3M9s3jfc7y8z+EXvM3c9191+n654i0pBCmkg4neDuXYG9gPXAH9J9Q3evBh4Gzog9bmZFwERgZrrL0BZmZsDpwCbgzDZeo2NKC5Vm+VZekUKlkCYSYkFwehQYHD1mZp3M7AYzW21m64MurBIz6wL8DegdtMJVmVlvMzvEzF4KWsnWmdmtZrZTglvOBL5rZjvHHBtD5N+av5nZj81suZltMbP3zOycRGVv3FJkZn8ys+tino8zsyVBuV40swNiXrvMzCqC+6wws9HNfExHAr2Bi4DTYt9b8LncaGYfmNlmM/tHcKx/UL6fmNlq4Dkz62Bm/xGc+7GZ/dnMugfX6Wxm95nZJ0F5XzOzXsFrZwWfxRYze9/MfthMWaPlWhg8/FdQT6cm8ZmsCj6XpcDnZtbRzKaa2bvBvd8ys5OCc/cH7gBGBNevTFAHPzOzlWa2yczmmFnvRvV3rpm9E7To3hYEYsxsXzN7PvhMN5rZwy29Z5FCpJAmEmJBWDoVeDnm8G+BrwOlwL5AH+Aqd/8cGAusdfeuwZ+1QB1wMbAHMAIYDZwf737u/iKwDjg55vDpwAPuXgt8DIwDdgF+DPzOzA5qw/s6CLgHOAfYHbgTmBME0EHAhcDB7t6NSEhc1czlzgT+QqQVkKB8UTcA3wQOB3YDfgnUx7x+FLB/cI+zgj+jgL2BrsCtMffoDvQLynsusC0IxrcAY4OyHg4saen9u/u3gocHBvX0cHOfScy3TgSOB3oE9fEukZDaHbgWuM/M9nL35UEZXwqu36NxGSzSrX098H0iLbYfAA81Om0ccDBwYHDemOD4r4H5wK5AXzLQ0iuSjxTSRMJpdtD68RlwNDADdnTt/Qy42N03ufsW4D+B0xJdyN0XufvL7l7r7quI/PI/qpl7/5mgy9PMdgFOJOjqdPe/uvu7HvE8kV/UR7bh/f0MuNPdX3H3OnefCWwHDiMSKjsBg82s2N1Xufu78S4ShNhTiITIGiKtjmcGr3UAzgYucveK4D4vuvv2mEtc4+6fu/s24IfATe7+nrtXAdOItMx1BGqIBKd9g+sscvfPgmvUA0PMrMTd17n7m234PFr6TKJucfcPg/Li7o+4+1p3r3f3h4F3gGTH5f0QuMfd/xl8JtOItLz1jzlnurtXuvtqYAGR/xhA5PP4GtDb3avdvcHYNxGJUEgTCacJQetHJyKtSs+b2Z7AV4CdgUVBl1gl8HRwPC4z+7qZPWVmH5nZZ0RC3R7N3PvPwCgz6wN8D1jp7ouDa401s5eD7rFK4LgWrpXI14BLou8huFY/Ir/0VwKTgWuAj83sodhuuEZOAmqBucHz+4GxZvaVoFydibQ2JfJhzOPeRFqToj4AOgK9gHuBecBDZrbWzP4rCJCfE2npPBdYZ5FJHvsl9Qk0lfAzSVBezOyMmO7RSmAIyddHg/cbBNNPiLTMRn0U83grkdZFiLRIGvCqmb1pZmcneU+RgqKQJhJiQYvK40Ral44ANgLbgG+4e4/gT/dgkgGAx7nMfwNvAwPdfRfgciK/YBPdczXwf0RaWk4nEtoIut0eI9KF2CsIkXObudZWIoEyas+Yxx8Cv4l5Dz3cfWd3fzAowwPufgSR4OJEunjjOZNIcFhtZh8BjwDFRLoFNwLVwD6J3isNP6+1wf2ivkokAK539xp3v9bdBxPp0hxH0Nro7vPc/WgiXYZvA3c3c7/mNPuZNC6vmX0tuNeFwO5BfSzjy/qI93chVoP3G3Td7g5UtFRQd//I3X/m7r2JdM/ebmmcqSqSrxTSRELMIk4kMvZnubvXE/nF/Dsz6xmc08fMomOF1gO7Rwe8B7oR6TatClp5zkvi1jOJ/PIfSaR1CmAnIi17G4BaMxsLHNPMNZYAPzCzIjM7loZdrHcD55rZocF77GJmx5tZNzMbZGbfDkJhNZFQWhfns+lDZHzdOCLdcKVExk79Fjgz+KzuAW6yyASKIjMb0WiMV6wHgYvNbICZdSXS4viwu9ea2SgzG2qRma6fEenuqzOzXmY2Pgg424GqeGVNYD2RsW8tfiYJvr8LkSC2Ifg8fkykJS32+n0t8SSRB4Afm1lp8Jn8J/BK0CXeLDM7xcz6Bk8/DcqR7PsWKRgKaSLh9BczqyISCH5DJHRExzpdBqwEXg66L58FBgG4+9tEwsZ7QRdYb+BS4AfAFiJBIJmZeI8SCYZ/d/d1wbW3AL8AZhH5xfwDYE4z17gIOAGoJNIqNzv6gru/TmQM1q3BtVYSGbQPkSA4nUhL2EdATyKtf42dDixx9/lBy85H7v4RkYH8B5jZkOC9vwG8RmSJjt+S+N/Ne4h0ay4E3icSEH8evLZn8Jl8BiwHngfuC651CZFWqU1EgmjcSRlxXAPMDOrp+y18Jk24+1vAjcBLRALZUOCFmFOeA94EPjKzjXG+/+/AlURaR9cRaXFMOLaxkYOBV4K/o3OIjPt7P8nvFSkY5t5Si7aIiIiIZJpa0kRERERykEKaiIiISA5SSBMRERHJQQppIiIiIjlIIU1EREQkB3XMdgHSYY899vD+/fun5dqff/45Xbp0Scu1JftUv+GnOg4/1XH4ha2OFy1atNHdm+z8EsqQ1r9/f15//fW0XLu8vJyysrK0XFuyT/Ubfqrj8FMdh1/Y6tjMPoh3XN2dIiIiIjlIIU1EREQkBymkiYiIiOQghTQRERGRHKSQJiIiIpKDFNJEREREcpBCmoiIiEgOUkgTERERyUEKaSIiIiI5SCFNREREJNbSWfC7IXBNj8jXpbOyUoxQbgslIiIiabZ0Fvz9V7D5Q7Ai8Lovv5bsFjln26bWH0vi/KO2bYLn03TP2u1Q8/mX73Pzh/CXX0QeH/D9NH2Y8SmkiYiIZFO8sNPG8NKu81t7DQzwyHGva/h126Yv319rjyVxvmXinrFqtkXqqJBCmpndA4wDPnb3IXFeN+D3wHHAVuAsd/9nZkspIiJ5b0cQWgMlu0aO5UoAihd22hhe2nV+a68RLXOh2Lwm47fMdkvan4BbgT8neH0sMDD4cyjw38FXERHJNxlqMWrQFRav+yrXAlChhZ181b1vxm+Z1ZDm7gvNrH8zp5wI/NndHXjZzHqY2V7uvi4zJRQRCblMtjBlqMXIEp0v0lbFJTD6qozfNtstaS3pA3wY83xNcKxJSDOzScAkgF69elFeXp6WAlVVVaXt2pJ9qt/wy7c67rn+efZ+7146bd+A0wGjnpqibmBQXLul2WMtnd+h/guKfHsk1ED6W5jUYiR5xoGajt1Yue/P+HhTT8jwvx25HtIszrG4P+XufhdwF8Dw4cO9rKwsLQUqLy8nXdeW7FP9hl/a6jjVLVJxuumMegB2qtuS1LFkzxdpu6B1NMOzO33bJiyt9/wUuvfFRl/FTgd8n8HA4LR+jvHlekhbA/SLed4XWJulsohI2CQ7Rqq1U/ZTPYZJCkCjsJMPszu794t0AWZ4xiPA8wXyH+pcD2lzgAvN7CEiEwY2azyaiABtDlhHed2Xg8qTHSPVlin7kruKu0DHTrkTgLIYdiS3ZXsJjgeBMmAPM1sDXA0UA7j7HcBcIstvrCSyBMePs1NSEUmbtiyI2bjlqhUBq8Ggco2RyoL0txg16Apr1H2lMCT5JNuzOye28LoDF2SoOCLSVu3pNmzPgpiSWuluYcpQi1GhdIVJ+OV6d6eIZENrQlc7WrWCB2l7G6FjHcDrU9wlpxYmkVylkCZSSFqagdie0CVfSkWLlEKUSMFTSBPJZ61Z9iGZGYgFGbqSGCPViin7ClMikioKaSK5qqUB9W1d9iGMWtEN6F735aByzaoTkRymkCaSLS21giU7oD4U2rIgZttarjSoXETyhUKaSDqkohUsXwfUt3Zwu1qzRETiUkgTaYtCagVLKnRpPJaISKoppIkkI7ZlLDaAQf62gsWbgajQJSKSMxTSRGBHCDtq8xp4pYWWsVwNYEkt+6DwJSKSLxTSpHAkWqA1ZnyYQY61jLU0oF6hS0QkrBTSJJwajxlLZoHWbIrXCqYB9SIiBU0hTfJfS4Esa0FMrWAiItJ2CmmSf5IdxJ8pagUTEZE0UEiT3BZvHFnWBvE3ahlTEBMRkTRSSJPc0VIgy8AgfgdMLWMiIpIDFNIkuxJ1XaY7kDVZoDUyPmx571MYfOrV6bmniIhIKyikSWY1N54sXYFsR8tYywP1Py4vZ3B6SiEiItIqCmmSPtkYT9aKQCYiIpLLFNIktTLWfalB/CIiEm4KaZIaS2fB3y5rtASGApmIiEhbKaRJ67W0eGy7KJCJiIiAQpq0RrzWslQuHluyG4z9rQKZiIgICmnSkmZnY7aVWstERERaopAm8aVsjJkCmYiISFsopMmXUt1qpu5LERGRNlNIk9TPzFQ4ExERaTeFtEIWN5y1khaPFRERSQuFtELT7i7N4Hs0tkxERCStFNIKRSq6NNWNKSIikjEKaWGmVjMREZG8pZAWVktnwV9+ATXbggNqNRMREcknCmlhtHQWPHFuzKbmraBwJiIikhOyGtLM7Fjg90AR8Ed3n97o9TLgSeD94NDj7v6rTJYxr7Rptqa6NEVERHJR1kKamRUBtwFHA2uA18xsjru/1ejU/3P3cRkvYL5oz7gztZqJiIjkrGy2pB0CrHT39wDM7CHgRKBxSJNEWj3uTK1mIiIi+SKbIa0P8GHM8zXAoXHOG2Fm/wLWApe6+5uZKFzOa+24MyuCk+5QMBMREckT2QxpFudY46agfwJfc/cqMzsOmA0MjHsxs0nAJIBevXpRXl6eupLGqKqqStu1W9Jz/fPs/d69dNq+AYj/AcZT16ETKwZdwMebekKWyp4vslm/khmq4/BTHYdfodRxNkPaGqBfzPO+RFrLdnD3z2IezzWz281sD3ff2Phi7n4XcBfA8OHDvaysLC2FLi8vJ13XbtbSWfDCf8d0bSapZDeKxv6WwQd8n8HpKVmoZK1+JWNUx+GnOg6/QqnjbIa014CBZjYAqABOA34Qe4KZ7Qmsd3c3s0OADsAnGS9ptrWqa1PjzkRERMIgayHN3WvN7EJgHpElOO5x9zfN7Nzg9TuA7wHnmVktsA04zd1bu2x+fotODkgmoGncmYiISGhkdZ00d58LzG107I6Yx7cCt2a6XDmjNS1oxSVwwi0KaCIiIiHRIdsFkASSakELpg5076eAJiIiEjLaFirXNFicthnq2hQREQk1hbRc0mRx2gTUtSkiIhJ6Cmm5ItnxZ1akgCYiIlIANCYtFyQ7g7O4RF2cIiIiBUItadmU7Pgz0LpnIiIiBUYhLVs0/kxERESaoZCWDRp/JiIiIi3QmLRM0/gzERERSYJCWqb9/Vctd3FqcVoREZGCp+7OTElmkoDGn4mIiEhAIS0TkpkkoPFnIiIiEkMhLd2SmSSgFjQRERFpRGPS0imZSQIafyYiIiJxqCUtnVqaJNC9H1y8LHPlERERkbyhlrR0WTqr5UkCo6/KXHlEREQkryikpUO0mzMRTRIQERGRFiikpUNz3ZxapFZERESSoJCWSktnwe+GNN/NqRY0ERERSYImDqRKMmuhde+ngCYiIiJJUUtaqrQ0k1MTBURERKQVFNJSZfOaxK9pLTQRERFpJXV3psLSWWAd4i9aq7XQREREpA3UktZeze0qoC5OERERaSOFtPZKNBZNa6GJiIhIOyiktVVLy214vQKaiIiItJnGpLVFUstt9M1ceURERCR01JLWFlpuQ0RERNJMIa0ttNyGiIiIpJm6O1up5/rntdyGiIiIpJ1a0lpj6SwGrbhNy22IiIhI2imktcbff0VR/famx7XchoiIiKSYQlprJBqLpuU2REREJMWyGtLM7FgzW2FmK81sapzXzcxuCV5famYHZaOcOyRaVkPLbYiIiEiKZS2kmVkRcBswFhgMTDSzwY1OGwsMDP5MAv47o4VsbPRV1HXo1PCYxqKJiIhIGmSzJe0QYKW7v+fuXwAPASc2OudE4M8e8TLQw8z2ynRBdzjg+6wYdEFkFiem5TZEREQkbbK5BEcfIHZPpTXAoUmc0wdY1/hiZjaJSGsbvXr1ory8PJVl3aGqyzf5eNhRXx7YBKTpXpJ5VVVVafu7I7lBdRx+quPwK5Q6zmZIszjHvA3nRA663wXcBTB8+HAvKytrV+ESKS8vJ13XluxT/Yaf6jj8VMfhVyh1nM3uzjVAv5jnfYG1bThHREREJHSyGdJeAwaa2QAz2wk4DZjT6Jw5wBnBLM/DgM3u3qSrU0RERCRsstbd6e61ZnYhMA8oAu5x9zfN7Nzg9TuAucBxwEpgK/DjbJVXREREJJOyunenu88lEsRij90R89iBCzJdLhEREZFs044DIiIiIjlIIU1EREQkB2W1uzMfvbi2hiumP8faym307lHClDGDmDCsT7aLJSIiIiGjkNYKsxdX8KdlX/BFfeR5ReU2pj3+BoCCmoiIiKSUujtbYca8FTsCWtS2mjpmzFuRnQKJiIhIaCmktcLaym2tOi4iIiLSVgpprdC7R0mrjouIiIi0lUJaK0wZM4idGn1iJcVFTBkzKDsFEhERkdBSSGuFCcP6cNaQnejdozMAO+9UxPUnD9WkAREREUk5hbRWOrx3MS9OHc3o/XqyZ/fOCmgiIiKSFgppbXTo3rvx3obP+fiz6mwXRUREREJIIa2NDh2wOwCvvL8pyyURERGRMFJIa6N3P96CAT9/cDEjpz/H7MUV2S6SiIiIhIhCWhvMXlzBFbPfxIPn0Z0HFNREREQkVRTS2mDGvBVsq6lrcEw7D4iIiEgqKaS1gXYeEBERkXRTSGsD7TwgIiIi6aaQ1gZTxgyipLiowbGS4g7aeUBERERSpmO2C5CPogvYzpi3goqgi/Pio7+uhW1FREQkZRTS2mjCsD5MGNaHVRs/p+yG8iYtayIiIiLtoe7Odvra7jvTp0cJ/1i5MdtFERERkRBRSGsnM6PfriXMf3M9A6b+VQvbioiISEqou7OdZi+uYNHqT5ssbAtojJqIiIi0mVrS2mnGvBXU1HmDY1rYVkRERNpLIa2dtLCtiIiIpINCWjtpYVsRERFJB4W0doq/sG2RFrYVERGRdtHEgXaKTg74r3lvs7aymp13KuI/TxqqSQMiIiLSLmpJS4EJw/rw4tTRHH/AXnTp1JHxB/bOdpFEREQkz7UY0sxsHzPrFDwuM7NfmFmPtJcsD+3SuSMbtmxnn8vnar00ERERaZdkWtIeA+rMbF/gf4ABwANpLVUemr24gif+GQllzpfrpSmoiYiISFskE9Lq3b0WOAm42d0vBvZKb7Hyz4x5K6iurW9wTOuliYiISFslE9JqzGwicCbwVHCsOH1Fyk9aL01ERERSKZmQ9mNgBPAbd3/fzAYA97Xnpma2m5k9Y2bvBF93TXDeKjN7w8yWmNnr7blnumm9NBEREUmlFkOau78FXAb8M3j+vrtPb+d9pwJ/d/eBwN+D54mMcvdSdx/eznumldZLExERkVRKZnbnCcAS4OngeamZzWnnfU8EZgaPZwIT2nm9rJswrA/XnzyUPkHLmQG/OvEbWi9NRERE2sTcvfkTzBYB3wbK3X1YcOwNdx/a5puaVbp7j5jnn7p7ky5PM3sf+JTIhMk73f2uZq45CZgE0KtXr28+9NBDbS1es6qqqujatWuz5yzdUMtNi7Yz+aBOlPbUesH5JJn6lfymOg4/1XH4ha2OR40atShej2EyCaLW3TebWeyx5pMdYGbPAnvGeemKJO4ZNdLd15pZT+AZM3vb3RfGOzEIcHcBDB8+3MvKylpxm+SVl5fT0rVH1NZx65J53LWslm1fbKd3jxKmjBmkVrU8kEz9Sn5THYef6jj8CqWOkwlpy8zsB0CRmQ0EfgG82NI3uft3Er1mZuvNbC93X2dmewEfJ7jG2uDrx2b2BHAIEDek5ZK/vfERtfXOF3V1wJdrpgEKaiIiIpKUZGZ3/hz4BrAdeBD4DJjczvvOIbKkB8HXJxufYGZdzKxb9DFwDLCsnffNiBnzVlDfqK1Ra6aJiIhIa7TYkubuW4l0Ubamm7Il04FZZvYTYDVwCoCZ9Qb+6O7HAb2AJ4Ju1o7AA+7+dArLkDZaM01ERETaq8WQZmYLiDMGzd2/3dabuvsnwOg4x9cCxwWP3wMObOs9sql3jxIq4gQyrZkmIiIiyUpmTNqlMY87A98FatNTnHCYMmYQ0x5/g201dTuOac00ERERaY1kujsXNTr0gpk9n6byhEJ0csBvn36bdZur6dqpI9dNGKJJAyIiIpK0ZBaz3S3mzx5mNob4S2tIjAnD+vDStNEM2rMb22rquPjhJYyc/hyzF1dku2giIiKSB5Lp7lxEZEyaEenmfB/4SToLFRazF1fw3oYq6oKpnlqKQ0RERJKVTHfngEwUJIxmzFtBTV3DORfRpTgU0kRERKQ5CUOamZ3c3De6++OpL064aCkOERERaavmWtJOaOY1BxTSWqClOERERKStEoY0d/9xJgsSRlqKQ0RERNoqmYkDmNnxRLaG6hw95u6/SlehwiI67mzGvBVUVG6jg8F1E76h8WgiIiLSomSW4LgDOJXIHp5GZAunr6W5XKExYVgfXpj6bX56xADqHS55ZKmW4hAREZEWJbPB+uHufgbwqbtfC4wA+qW3WOEye3EF973ywY7n0aU4FNREREQkkWRCWnTk+9ZgA/QaQMtytMKMeSuorqlvcCy6FIeIiIhIPMmMSXvKzHoAM4B/EpnZeXc6CxU2WopDREREWqu5ddL+CjwA3OTunwOPmdlTQGd335ypAoaBluIQERGR1mquu/MuYBzwvpk9bGYTAFdAa70pYwZRUlzU4Finjh20FIeIiIgklDCkufuT7j6RyEzOx4EzgdVmdo+ZHZ2pAobBhGF9uP7kofTpUYIFxwy06bqIiIgk1OLEAXff5u4Pu/tJwDHAMODptJcsZKJLcfzu1FI6GFTX1uNopqeIiIjEl8w6ab3M7Odm9gIwG5gPfDPdBQurGfNWUN9wz3XN9BQREZEmmps48DNgIjCISHfnL939hUwVLKw001NERESS0dwSHIcD04Fn3b2+mfOkFTTTU0RERJLR3MSBH7v7fAW01Io301ObrouIiEhjSW2wLqnTeNN1aDgmTZuvi4iICCS3LZSk2IRhfZgyZhA7FX358WuWp4iIiMRKOqSZWU8z+2r0TzoLVQhmzFvBF3Xaz1NERETiS2YJjvFm9g7wPvA8sAr4W5rLFXqa5SkiIiLNSaYl7dfAYcC/3X0AMBrQUhztlGg2p2Z5ioiICCQX0mrc/ROgg5l1cPcFQGl6ixV+8WZ5dtZ+niIiIhJIJqRVmllXYCFwv5n9HqhNb7HCL95+nqD9PEVERCQimZB2IrAVuJjInp3vAuPSWahCof08RUREJJFkQtpV7l7v7rXuPtPdbwEuS3fBCon28xQREZHGkglpR8c5NjbVBSlkmukpIiIijTW3wfp5wPnA3ma2NOalbmh2Z0ppP08RERFprLmWtAeAE4A5wdfon2+6+4/ac1MzO8XM3jSzejMb3sx5x5rZCjNbaWZT23PPXBZvpifA1i9qNS5NRESkQDW3wfpmd1/l7hOBfsC33f0DIktxDGjnfZcBJxOZMRqXmRUBtxHpWh0MTDSzwe28b06KzvTsUVLc4PinW2s0gUBERKRAJbPjwNVEJgpMCw7tBNzXnpu6+3J3b2lU/CHASnd/z92/AB4iMtM0lCYM60OXTk17nzWBQEREpDAlM3HgJGA88DmAu68lMi4t3foAH8Y8XxMcCy1NIBAREZGohBMHYnzh7m5mDmBmXZK5sJk9C+wZ56Ur3P3JZC4R55jHORa93yRgEkCvXr0oLy9PppitVlVVlbZr79bZ+KS66VvcrbOl7Z7SUDrrV3KD6jj8VMfhVyh1nExIm2VmdwI9zOxnwNnA3S19k7t/p51lW0NkLFxUX2BtM/e7C7gLYPjw4V5WVtbO28dXXl5Ouq59ZfcKpj3+Bttq6hoc/6TaueLleqaMGcSEYaFuTMy6dNav5AbVcfipjsOvUOq4xZDm7jeY2dHAZ8AgIovbPpP2ksFrwMBgkkIFcBrwgwzcN2uiAWzGvBVNluSI7kIQe56IiIiEVzJj0nD3Z9x9CjAdeLa9NzWzk8xsDTAC+KuZzQuO9zazucE9a4ELgXnAcmCWu7/Z3nvnuuhWUX3irJGmSQQiIiKFo7nFbA8jEso2Ab8G7gX2ILIExxnu/nRbb+ruTwBPxDm+Fjgu5vlcYG5b75PPNIlARESksDXXknYr8J/Ag8BzwE/dfU/gW8D1GShbQUu020AHM62bJiIiUgCaC2kd3X2+uz8CfOTuLwO4+9uZKVphS7QLQZ27FrgVEREpAM2FtPqYx4372BIuhSGpEd2FoMiarkSisWkiIiLh11xIO9DMPjOzLcABwePo86EZKl9BmzCsD/UePw9rbJqIiEi4Nbd3Z5G77+Lu3dy9Y/A4+rw40fdJamlsmoiISGFKagkOyR6NTRMRESlMCmk5TmPTRERECpNCWh7Q2DQREZHCo5CWJxKNTXNg5PTn1O0pIiISMgppeSLR2DT4cl9PBTUREZHwUEjLE9GxafH29ASNTxMREQkbhbQ8Et18vekUggiNTxMREQkPhbQ8pLXTREREwk8hLQ9p7TQREZHwU0jLQ1o7TUREJPwU0vKU1k4TEREJN4W0PKa100RERMJLIS2Pae00ERGR8FJIy2NaO01ERCS8FNLyXEtrp1VUblNrmoiISB5SSAuJROPTAHV7ioiI5CGFtJBobnyauj1FRETyj0JaSETHpyWibk8REZH8opAWIhOG9Uk4iQDU7SkiIpJPFNJCRt2eIiIi4aCQFjLJdHtqoVsREZHcp5AWQi11e2qhWxERkdynkBZSzXV7gro+RUREcp1CWki1tBsBaManiIhILlNIC7HobgSa8SkiIpJ/FNIKQEszPic/vESTCURERHJMx2wXQNJvwrA+AEx+eEnCc6KTCWLPFxERkexRS1qBaGnGJ2gygYiISC7JSkgzs1PM7E0zqzez4c2ct8rM3jCzJWb2eibLGEYtzfgETSYQERHJFdnq7lwGnAzcmcS5o9x9Y5rLUxCi3Zgz5q2gonJbwvPU7SkiIpJ9WWlJc/fl7q5+tSyIzvi8+dTSZicTXDLrX2pRExERyaJcH5PmwHwzW2Rmk7JdmDBpafuoOnctzyEiIpJF5u7pubDZs8CecV66wt2fDM4pBy5197jjzcyst7uvNbOewDPAz919YYJzJwGTAHr16vXNhx56KAXvoqmqqiq6du2almtnwyXlW/mkOvHfgd07GzeW7ZzBEmVX2OpXmlIdh5/qOPzCVsejRo1a5O5NxuinLaQlo6WQ1ujca4Aqd7+hpXOHDx/ur7+ennkG5eXllJWVpeXa2TB7cQXTHn+DbTV1Cc/p06OEKWMGFcQYtbDVrzSlOg4/1XH4ha2OzSxuSMvZ7k4z62Jm3aKPgWOITDiQFIp2exaZJTxHG7KLiIhkXraW4DjJzNYAI4C/mtm84HhvM5sbnNYL+IeZ/Qt4Ffiruz+djfKG3YRhfbjx+we2uCG7JhOIiIhkTlaW4HD3J4An4hxfCxwXPH4PODDDRStYySzPEZ1MEHu+iIiIpEfOdndK5iWzIbv2+hQREckMhTRpItmdCTROTUREJH0U0qSJZCYTgMapiYiIpJNCmsSVzGQC0KK3IiIi6aKQJglFW9SaG6MGGqcmIiKSDgpp0qxk9vqM0jg1ERGR1FFIk6RonJqIiEhmKaRJ0jROTUREJHOyspit5K9kFr2FL1vUYr9HREREkqeWNGm1ZMepqUVNRESk7RTSpM2SGaemmZ8iIiJto5Am7ZLsOLWKym1MfngJw341X2FNREQkCQpp0m7JzvwE+HRrjbpARUREkqCQJimRbIsaqAtUREQkGQppkjLJ7lAQpS5QERGRxBTSJKVas0NBlLpARUREmlJIk7SItqr1KClO6nztVCAiItKQQpqkzYRhfVhy9THcfGppUl2gde5c/PAS+k/9q8ariYhIwVNIk7RrTReoB1+1WbuIiBQ6hTTJGHWBioiIJE8hTTIqtgs0mXXV1AUqIiKFSiFNsqI166rFdoFqyQ4RESkUCmmSNY3XVWu5XS3i0601al0TEZHQ65jtAkhhmzCsDxOG9QFg9uIKLpn1L+rcW/iuphMMotcSEREJC7WkSc5oTRdoLE0wEBGRMFJIk5zS1i7QOneNVxMRkVBRd6fknMZdoNfMeZPKbTVJfW90vNrkh5fQp0cJU8YMUjeoiIjkJbWkSU6Lt2tBS61rmg0qIiJhoJY0yQttnWAAal0TEZH8pJY0yTttmWCg1jUREck3CmmSl1q7xVRjn26tUVgTEZGcpu5OyVvRLtDZiyuYMW8FFZXbML5sNUtG467Q479aR1mayisiItIaCmmS99ozGxQadoXeVQmPrJzP1Sd8Q+PWREQkq7LS3WlmM8zsbTNbamZPmFmPBOcda2YrzGylmU3NcDElD7VlNmhj2nZKRERyQbbGpD0DDHH3A4B/A9Man2BmRcBtwFhgMDDRzAZntJSStyYM68MLU7/NqunH87tTS1s9dk0TDUREJNuyEtLcfb671wZPXwb6xjntEGClu7/n7l8ADwEnZqqMEh6xrWvtmWig1jUREckk8yTXmkpbAcz+Ajzs7vc1Ov494Fh3/2nw/HTgUHe/MMF1JgGTAHr16vXNhx56KC3lraqqomvXrmm5tmTGi2treOzfNXxS3b6/+9FJCrt3Nr779WIO7922ACiZpZ/h8FMdh1/Y6njUqFGL3H144+NpC2lm9iywZ5yXrnD3J4NzrgCGAyd7o4KY2SnAmEYh7RB3/3lL9x4+fLi//vrr7X0LcZWXl1NWVpaWa0vmtWWiQTzRwKbFcnOffobDT3UcfmGrYzOLG9LSNrvT3b/TQoHOBMYBoxsHtMAaoF/M877A2tSVUCQ1y3hA0zFs1/7lTc0QFRGRdsnKEhxmdixwGXCUu29NcNprwEAzGwBUAKcBP8hQEaXARMNaeXk5ld0Htrt1LXb9tSIz6tzVyiYiIq2SrdmdtwLdgGfMbImZ3QFgZr3NbC5AMLHgQmAesByY5e5vZqm8UkBSsYwHfNm6Ft1jVDNFRUSkNbLSkubu+yY4vhY4Lub5XGBupsolEqu9i+QmolY2ERFJhvbuFElCvNa1Iou0r6WqlU1LfIiISCxtCyXSCrGta1GpamVrPPng8seX0qm4iMqtNfRWS5uISMFRS5pIO6VqDFtjW2vq+XRrDY7Gs4mIFCK1pImkSOMxbO1Z0iORT7fWqJVNRKRAKKSJpEHjbtFUh7atNfVsrakH1D0qIhJWCmkiGZColS06uzNdwe3/zVpCvWsnBBGRfKSQJpJh6Zx80Fh9kPyis0cnP7xEgU1EJE8opInkgMbbU62t3Eb3kmK+qK3b0TrWXpo9KiKSXxTSRHJIJlvZmhvX9unWGi20KyKSZQppIjkuE61sUbHBrfF2Vmp5ExHJLIU0kTyRqJUt3cEtSjNKRUQySyFNJI81F9xSPXs0Hs0oFRFJH4U0kZDJ5Li2eOLNKI2GxR4lxZihljcRkSQopIkUgEyOa4vVeCP52JCoLlMRkeYppIkUkGTGtZlFtp9KVxdpY811marlTUQKWcGEtJqaGtasWUN1dXW7rtO9e3eWL1+eolJJpnTu3Jm+fftSXFyc7aLknHjBLSrTLW9R0S7T5lretEyIiIRdwYS0NWvW0K1bN/r374+Ztfk6W7ZsoVu3biksmaSbu/PJJ5+wZs0aBgwYkO3i5JVszyiNJ5llQhTgRCQMCiakVVdXtzugSX4yM3bffXc2bNiQ7aKEQrZnlDYnUYBrPIGhaN5cBTgRyXkFE9IABbQCprpPr2SDW48stLxB0wkMzQW4HjHj8tQaJyLZ1CHbBSgkRUVFlJaWcuCBB3LQQQfx4osvNnt+ZWUlt99+e4vXXbVqFSUlJZSWlu449vTTTzNo0CD23Xdfpk+fvuP4lClT2HPPPbnhhhviXmvy5MksXLgQgLKyMl5//fVm7z179mzeeuutFsuYLjfffDNbt27d8fw73/kOn376adbKI1+aMKwPL0z9NqumH8+71x/HqunHs+TqY3jr12O5+dRS+vQowYhMDti5OHv/FMWbgfrp1poGx6Jhrv/Uv7LPtLkNvo6c/hyzF1dko+giEnIF1ZLWGrHjblI1q6ykpIQlS5YAMG/ePKZNm8bzzz+f8PxoSDv//PNbvPY+++yz49p1dXVccMEFPPPMM/Tt25eDDz6Y8ePHM3jwYGbMmEGXLl3iXmPTpk28/PLL3HzzzUm/p9mzZzNu3DgGDx6c9PfU1tbSsWNq/urdfPPN/OhHP2LnnXcG4PTTT+f222/niiuuSMn1JT1yveUtHrXGiUimKaTFMXtxBdMef4NtNXVA5B/haY+/AcDofXdJyT0+++wzdt111x3PZ8yYwaxZs9i+fTsnnXQS1157LVOnTuXdd9+ltLSUo48+mquvvpoTTzyRTz/9lJqaGq677jpOPPHEJtd+9dVX2Xfffdl7770BOO2003jyySdbDFKPPvooxx57bNzXunbtykUXXcRTTz1FSUkJTz75JO+++y5z5szh+eef57rrruOxxx4D4IILLmDDhg3svPPO3H333ey3336cddZZ7LbbbixevJiDDjqI8847j3PPPZcNGzZQVFTEI488wj777BP3c1i1ahXHHnsshx56KIsXL+brX/86f/7zn/njH//I2rVrGTVqFHvssQcLFixg/PjxHHnkkQppeag1s0wzvUxIS5pbDy6ZyQ1aakRE4inIkHbtX97krbWfJXx98epKvqhr+D/3bTV1/PLRpRzQpxtFRUVNvmdw7124+oRvNHvfbdu2UVpaSnV1NevWreO5554DYP78+bzzzju8+uqruDvjx49n4cKFTJ8+nWXLlu1oIautreWJJ55gl112YePGjRx22GGMHz++yX0qKiro16/fjud9+/bllVdeabZsAC+88ALf+9734r72+eefc9hhh/Gb3/yGX/7yl9x99938x3/8B+PHj2fcuHE7vm/06NHccccdDBw4kFdeeYXzzz9/x/v897//zbPPPktRURGHHnooU6dO5aSTTqK6upr6+vqEn8NXv/pVVqxYwf/8z/8wcuRIzj77bG6//XYuvfRSbrrpJhYsWMAee+wBwK677sr27dv55JNP2H333Vt8z5If2hLgsjmBoTnxJjcku9SIWuVECktBhrSWNA5oLR1PVmx350svvcQZZ5zBsmXLmD9/PvPnz2fYsGEAVFVV8c477/DVr361wfe7O5dffjkLFy6kQ4cOVFRUsH79+ib3cW/6KymZgfPr1q3jK1/5StzXdtppJ8aNGwfAN7/5TZ555pkm51RVVfHiiy9yyimn7Di2ffv2HY9POeUUioqK2LJlCxUVFZx00klAZA0zoNnPoV+/fowcORKAH/3oR9xyyy1ceumlccvas2dP1q5dq5BWIJoLcBC/GzVXA1yseGGutV2sCnUi+a0gQ1pLLV4jpz9HReW2Jsf79Cjhf08/MCXrpI0YMYKNGzeyYcMG3J1p06ZxzjnnNDhn1apVDZ7ff//9bNiwgUWLFlFcXEz//v3jLs7bt29fPvzwwx3P16xZQ+/evVssU0lJScLFfouLi3cEvaKiImpra5ucU19fT48ePXYE0caiY+Hihcjo8USfQ+OQ2VzorK6upqSkJOHrUlgah7jy8nLKysqAxOPgcr01DpLrYm1tqFO3q0huKciQ1pIpYwY1GJMGUFJcxJQxg1J2j7fffpu6ujp23313xowZw5VXXskPf/hDunbtSkVFBcXFxXTr1o0tW7bs+J7NmzfTs2dPiouLWbBgAR988EHcax988MG88847vP/++/Tp04eHHnqIBx54oMUy7b///qxcuXLHL7BkxJZxl112YcCAATzyyCOccsopuDtLly7lwAMPbPA9u+yyC3379mX27NlMmDCB7du3U1dXl/BzAFi9ejUvvfQSI0aM4MEHH+SII45ocP9od6e789FHH9G/f/+k34MUrpZa4aLytTUuVjKhLl63a+MtuhTqRDJHIS2O6D8y8WZ3xoam1oqOSYNImJg5cyZFRUUcc8wxLF++nBEjRgCRQfr33Xcf++yzDyNHjmTIkCGMHTuWyy67jBNOOIHhw4dTWlrKfvvtF/c+HTt25NZbb2XMmDHU1dVx9tln841vNN96CHD88cdz55138tOf/jTp93Taaafxs5/9jFtuuYVHH32U+++/n/POO4/rrruOmpoaTjvttCYhDeDee+/lnHPO4aqrrqK4uJhHHnkk4edQVFTE/vvvz8yZMznnnHMYOHAg5513HgCTJk1i7Nix7LXXXixYsIBFixZx2GGHpWz2qAgkNyYuUWtcvoS4ROJt0aVQJ5IZlqjrKZ8NHz7cG6/vtXz5cvbff/92XzsXt4VatWoV48aNY9myZUmdf80119C1a9e4Y7qOOOIInnrqKXr06JHiUrZda97fRRddxPjx4xk9enST15L5OxDbFSbhlI06bmlyQy4tNZJpOxd3SDhBoq1LmejnOPzCVsdmtsjdhzc+ruaGECgqKmLz5s2UlpYmHA8WNWXKFJ544gkuueSSuK/feOONrF69OqdCWmsMGTIkbkATyabWdqsmCnP51sWajOYmSCRayqTJNl/xQl2w9ZfWrJN8ppa0VsrFljRJjlrSBMJRx8lOeAhjqEuV6GeSTOudumlzTxh+jmOpJU1EJCSSbZmLSjbUFVK3a6IdJFI59q6lY2rRk5YopImIhJxCXeolO6GivbtQpHKsnuQfhTQREWlAoS6zWtqFIqVj9dTal1eyEtLMbAZwAvAF8C7wY3evjHPeKmALUAfUxuuvFRGR7GprqGtpgkS+LCycK9rahZup1j6FwNbLVkvaM8A0d681s98C04DLEpw7yt03Zq5o6VNUVMTQoUNxd4qKirj11ls5/PDDE55fWVnJAw88wPnnn9/sdVetWsX+++/PoEGDdszuPPvss3nqqafo2bNng6UrpkyZwr333sull14adwmOyZMnc/LJJ/Otb32Lp556iiuvvJL6+npqamq46KKLOOecc5pdwqOt3njjDW688Ub+9Kc/peyaIpKbWhvqYjW3sHCywUBBr/VS0dqX8hbAYAZvmCd9ZCWkufv8mKcvA/F39c6mpbPg77+CzWuge18YfRUc8P12XTJ278558+Yxbdo0nn/++YTnV1ZWcvvtt7cY0gD22WefBstvnHXWWVx44YWcccYZDc6bMWPGju2ZGtu0aRMvv/wyN998MzU1NUyaNIlXX32Vvn37sn379ibbVKXS0KFDWbNmDatXr26yZ6mISFQyAa+lmX9tCXrqpk2f9rYAtnbSR7xWwVxt7cuFMWlnAw8neM2B+WbmwJ3ufleii5jZJGASQK9evSgvL2/wevfu3ZPeLaDj8ifoPP+XWG2wf+fmD/E5v6C6upq6r49v164D0e9dv359gy2Vfv/73/P444/zxRdfMG7cOK644gouueQS3n33XQ444ABGjRrF1KlTmThxIpWVldTU1HDllVdy/PHHU1VVRX19fYNyDRs2jA8++KDJcYhsel5cXNzk+H333ceoUaPYsmULmzZtoqamhp122mnHeb1792bLli0Nvn/p0qVMnjyZbdu2MWDAAG677TZ23XVXjjvuOIYOHcqiRYvYsmULt912G8OHD+fzzz9nypQpvPnmm9TV1TFt2jSOP/54AI4++mhmzpzJ5MmT2/z5Nqe6urrJ34vGqqqqWjxH8pvqOPxaquMewG8O6wDE/w9rxE5Njry4tobH/l3DJ9VOB6Ae6NIRzKCqhqSPSXa1ZW2+Xz6yhLeWv8XhvYszWta0hTQzexbYM85LV7j7k8E5VwC1wP0JLjPS3deaWU/gGTN7290XxjsxCHB3QWSdtMb/i1q+fPmX65v9bSp89Ebiwq95Deq2N3w/tdsomXcpxUvvp2NRnI9tz6EwdnriaxLZFurII4+kurqadevW8dxzz9GtWzfmz5/P6tWrWbRoEe7O+PHjWbx4MTfeeCMrVqxg6dKlANTW1jJnzhx22WUXNm7cyGGHHcapp55K165d6dChQ5P12xId79SpE506dWpyfNGiRXzve9+jW7dudOvWjRNPPHHH4rDjxo1j4sSJdOjQocH3n3feefzhD3/gqKOO4qqrruKmm27i5ptvpqioiJqaGl555RUWLlzI+eefz7Jly7j++usZM2YM9957L5WVlRxyyCGccMIJdOnShSOOOILp06enbR26zp07M2zYsGbPCdvaO9KU6jj80lXHZcDlKbpWMrtQaKxe7viiHv66uojLf1CW0fumLaS5+3eae93MzgTGAaM9wYq67r42+PqxmT0BHALEDWkp1SigtXg8SbHdnS+99BJnnHEGy5YtY/78+cyfP39HgKiqquKdd95p0u3n7lx++eUsXLiQDh06UFFRwfr169tVpljr1q3jK1/5yo7nf/zjH3njjTd49tlnueGGG3jmmWcajBnbvHkzlZWVHHXUUQCceeaZnHLKKTtenzhxIgDf+ta3+Oyzz6isrGT+/PnMmTOHG264AYi0bq1evZr999+fnj17snbt2pS9HxGRXNWecXnxpGKsXpj2nE2HtZXbMn7PbM3uPJbIRIGj3H1rgnO6AB3cfUvw+BjgVykpQAstXvxuCGz+sOnx7v3YduqjKWnpGTFiBBs3bmTDhg24O9OmTeOcc85pcE7jMWD3338/GzZsYNGiRRQXF9O/f3+qq6vbXZaokpKSJtcbOnQoQ4cO5fTTT2fAgAGtGthvZk2euzuPPfYYgwYNanJ+dXU1JSUlbSq7iEghS3Xog9S39uV7C2DvHpn//ZStMWm3Ap2IdGECvOzu55pZb+CP7n4c0At4Ini9I/CAuz+dkdKNvgr+8guoiUnNxSWR4yny9ttvU1dXx+67786YMWO48sor+eEPf0jXrl2pqKiguLi4wZg1iLRc9ezZk+LiYhYsWMAHH3yQsvIA7L///qxcuZKysjKqqqp4/fXXd3QZLFmyhK997WsNzu/evTu77ror//d//8eRRx7Jvffeu6NVDeDhhx9m1KhR/OMf/6B79+50796dMWPG8Ic//IE//OEPmBmLFy/e0YL473//myFDhqT0PYmISNukI/g1looWwExM+igpLmLKmKaNC+mWrdmd+yY4vhY4Lnj8HnBgJsu1Q3QWZ7zZne2YNLBt2zZKS0uBSNflzJkzKSoq4phjjmH58uWMGDECiIwlu++++9hnn30YOXIkQ4YMYezYsVx22WWccMIJDB8+nNLSUvbbb7+E95o4cSLl5eVs3LiRvn37cu211/KTn/yk2fIdf/zx3Hnnnfz0pz/F3fmv//ovzjnnHEpKSujSpUvcVrSZM2dy7rnnsnXrVvbee2/+93//d8dru+66K4cffjifffYZ99xzDwBXXnklkydP5oADDsDd6d+/P0899RQACxYs2DGJQEREwq+tQbC14w7bszZfoc/uzE0HfL/dS240VldXl/C1iy66iIsuuqjJ8QceeKDB85deeqnJOfGWxnjwwQdbXb4jjzySadOmUVlZSY8ePZg7d27c86655podj0tLS3n55Zfjnvfd736X66+/vsGxkpIS7rzzzibnbt++nddff52bb7651eUWERFpTiZaBdOhQ7YLIO1XVFTE5s2bd7TSNWfKlCncd999CddKu/HGG1m9enWKS9iy1atXM336dDp21P8bREREQC1podCvXz8+/DDORIc4ZsyYwYwZMxK+fuihh6akTK1dh2rgwIEMHDgwJfcWEREJA7WkiYiIiOSgggppCZZjkwKguhcRkXxTMCGtc+fOfPLJJ/plXYDcnU8++YTOnTtnuygiIiJJK5gxaX379mXNmjVs2LChXdeprq7WL/s81LlzZ/r27ZvtYoiIiCStYEJacXExAwYMaPd1ysvLW9z/UURERKS9Cqa7U0RERCSfKKSJiIiI5CCFNBEREZEcZGGc7WhmG4DU7j7+pT2AjWm6tmSf6jf8VMfhpzoOv7DV8dfc/SuND4YypKWTmb3u7sOzXQ5JD9Vv+KmOw091HH6FUsfq7hQRERHJQQppIiIiIjlIIa317sp2ASStVL/hpzoOP9Vx+BVEHWtMmoiIiEgOUkuaiIiISA5SSEuSmR1rZivMbKWZTc12eSQ1zGyVmb1hZkvM7PXg2G5m9oyZvRN83TXb5ZTkmdk9ZvaxmS2LOZawTs1sWvBzvcLMxmSn1JKsBPV7jZlVBD/HS8zsuJjXVL95xsz6mdkCM1tuZm+a2UXB8YL7OVZIS4KZFQG3AWOBwcBEMxuc3VJJCo1y99KY6dxTgb+7+0Dg78FzyR9/Ao5tdCxunQY/x6cB3wi+5/bg511y159oWr8Avwt+jkvdfS6ofvNYLXCJu+8PHAZcENRlwf0cK6Ql5xBgpbu/5+5fAA8BJ2a5TJI+JwIzg8czgQnZK4q0lrsvBDY1OpyoTk8EHnL37e7+PrCSyM+75KgE9ZuI6jcPufs6d/9n8HgLsBzoQwH+HCukJacP8GHM8zXBMcl/Dsw3s0VmNik41svd10HkHwugZ9ZKJ6mSqE71sx0eF5rZ0qA7NNoNpvrNc2bWHxgGvEIB/hwrpCXH4hzTtNhwGOnuBxHpyr7AzL6V7QJJRulnOxz+G9gHKAXWATcGx1W/eczMugKPAZPd/bPmTo1zLBT1rJCWnDVAv5jnfYG1WSqLpJC7rw2+fgw8QaSJfL2Z7QUQfP04eyWUFElUp/rZDgF3X+/ude5eD9zNl11dqt88ZWbFRALa/e7+eHC44H6OFdKS8xow0MwGmNlORAYozslymaSdzKyLmXWLPgaOAZYRqdszg9POBJ7MTgklhRLV6RzgNDPrZGYDgIHAq1kon7RD9Bd34CQiP8eg+s1LZmbA/wDL3f2mmJcK7ue4Y7YLkA/cvdbMLgTmAUXAPe7+ZpaLJe3XC3gi8u8BHYEH3P1pM3sNmGVmPwFWA6dksYzSSmb2IFAG7GFma4CrgenEqVN3f9PMZgFvEZlRdoG712Wl4JKUBPVbZmalRLq4VgHngOo3j40ETgfeMLMlwbHLKcCfY+04ICIiIpKD1N0pIiIikoMU0kRERERykEKaiIiISA5SSBMRERHJQQppIiIiIjlIIU1EQsXMqoKv/c3sBym+9uWNnr+YyuuLiMRSSBORsOoPtCqkmVlRC6c0CGnufngryyQikjSFNBEJq+nAkWa2xMwuNrMiM5thZq8FG3GfA2BmZWa2wMweAN4Ijs02s0Vm9qaZTQqOTQdKguvdHxyLttpZcO1lZvaGmZ0ac+1yM3vUzN42s/uD1dQxs+lm9lZQlhsy/umISM7TjgMiElZTgUvdfRxAELY2u/vBZtYJeMHM5gfnHgIMcff3g+dnu/smMysBXjOzx9x9qpld6O6lce51MpHNvQ8E9gi+Z2Hw2jDgG0T2EnwBGGlmbxHZvmg/d3cz65Haty4iYaCWNBEpFMcAZwTbzLwC7E5kjz+AV2MCGsAvzOxfwMtENm4eSPOOAB4MNvleDzwPHBxz7TXB5t9LiHTDfgZUA380s5OBre18byISQgppIlIoDPi5u5cGfwa4e7Ql7fMdJ5mVAd8BRrj7gcBioHMS105ke8zjOqCju9cSab17DJgAPN2K9yEiBUIhTUTCagvQLeb5POA8MysGMLOvm1mXON/XHfjU3bea2X7AYTGv1US/v5GFwKnBuLevAN8CXk1UMDPrCnR397nAZCJdpSIiDWhMmoiE1VKgNui2/BPweyJdjf8MBu9vINKK1djTwLlmthRYQaTLM+ouYKmZ/dPdfxhz/AlgBPAvwIFfuvtHQciLpxvwpJl1JtIKd3Gb3qGIhJq5e7bLICIiIiKNqLtTREREJAcppImIiIjkIIU0ERERkRykkCYiIiKSgxTSRERERHKQQpqIiIhIDlJIExEREclBCmkiIiIiOej/A5CQmiO3K3JkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.484393\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                    5\n",
      "Model:                          Logit   Df Residuals:                        3\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Thu, 27 Mar 2025   Pseudo R-squ.:                  0.2803\n",
      "Time:                        07:11:00   Log-Likelihood:                -2.4220\n",
      "converged:                       True   LL-Null:                       -3.3651\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.1696\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -2.6486      2.806     -0.944      0.345      -8.148       2.851\n",
      "x1             1.0904      0.975      1.119      0.263      -0.820       3.001\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initial value for beta\n",
    "be = np.array([[0], [0]])\n",
    "\n",
    "# Tolerance\n",
    "eps = 0.001\n",
    "\n",
    "# Response variable\n",
    "y = np.array([[1], [1], [0], [1], [0]])\n",
    "\n",
    "# Corrected Predictor matrix (first column is 1s for the intercept, second column is the predictor values)\n",
    "x = np.array([[1, 5], [1, 2], [1, 1], [1, 4], [1, 3]])\n",
    "\n",
    "# Initialize iteration count and storage for beta values\n",
    "iteration = 0\n",
    "beta_values = []\n",
    "\n",
    "# Loop to iteratively compute the beta values until the tolerance condition is satisfied\n",
    "while True:\n",
    "    iteration += 1\n",
    "\n",
    "    # Fitted probabilities (logistic function)\n",
    "    p = 1 / (1 + np.exp(-np.dot(x, be)))\n",
    "    \n",
    "    # Variance and weights (diagonal matrix)\n",
    "    v = p * (1 - p)\n",
    "    w = np.diagflat(1 / v.flatten())\n",
    "\n",
    "    # Gradient and Hessian\n",
    "    z = np.dot(x.T, y - p)\n",
    "    z1 = np.dot(np.dot(x.T, w), x)\n",
    "\n",
    "    # Update beta using Newton-Raphson method\n",
    "    be_new = be + np.linalg.solve(z1, z)\n",
    "\n",
    "    # Store the beta values at each iteration for plotting\n",
    "    beta_values.append(be_new.flatten())\n",
    "\n",
    "    # Check if the absolute difference between successive betas is less than the tolerance\n",
    "    if np.all(np.abs(be - be_new) < eps):\n",
    "        break  # Stop the iteration if the tolerance condition is met\n",
    "\n",
    "    # Update beta for the next iteration\n",
    "    be = be_new\n",
    "\n",
    "# Output the final estimated coefficients\n",
    "print(f\"Converged after {iteration} iterations\")\n",
    "print(\"Final Estimated Beta values:\", be_new)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the list of beta values to a numpy array for easy indexing\n",
    "beta_values = np.array(beta_values)\n",
    "\n",
    "# Plotting the number of iterations vs. beta values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot beta[0] (first coefficient) and beta[1] (second coefficient) across iterations\n",
    "plt.plot(range(1, iteration + 1), beta_values[:, 0], label=\"Beta[0] (Intercept)\", marker='o')\n",
    "plt.plot(range(1, iteration + 1), beta_values[:, 1], label=\"Beta[1] (Slope)\", marker='o')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Beta Values')\n",
    "plt.title('Beta Values Across Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# For GLM output using statsmodels\n",
    "# Use the full matrix x (with intercept term included) for logistic regression\n",
    "model = sm.Logit(y, x)  # Note: x includes the constant term (all 1's in the first column)\n",
    "fitLR = model.fit()\n",
    "\n",
    "# Print the GLM coefficients\n",
    "print(fitLR.summary())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVaDYtmoZogDYzk6ShA0F9",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
