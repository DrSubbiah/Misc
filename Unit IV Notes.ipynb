{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPYYKlBI5sPL"
   },
   "source": [
    "<span style=\"font-size:15px\">\n",
    "    \n",
    "# <font color = maroon> Fitting Linear Model\n",
    "\n",
    "    \n",
    "## <font color = blue>Points to Ponder\n",
    "\n",
    "- Major aim is to understand ways to build a mathematical relation / mapping from a contextual possible relationships between quantities involved in a real time situation\n",
    "    \n",
    "\n",
    "- It is possible to construct different forms of such mapping in an explicit mathematical form \n",
    "    \n",
    "\n",
    "- Liner Model (LM) - A straightline (Linear) mapping is an easiest way to build mappings / models. However, it is possible to go beyond such modeling to construct Non-Linear (Curvilinear) models\n",
    "    \n",
    "\n",
    "- Symbollically, such  models are represented as $Y=f(X)$ where $Y$ is the response variable and $X$ is a set of predictors identified through a contextual mapping from a study / data. $f$ is the relation / mapping /  function between $X$ and $Y$. Generally, such practices are termed as Modeling\n",
    "    \n",
    "\n",
    "- There are two major types of modeling, depends majorly on the objective of the problem at hand; either to develop model  to **understand (quantify)** the possible relationship between $x$ and $Y$ or the interest is **only to predict** value of $Y$ for a given $X$. Thi notes confines to the first modeling practice - interpretable or explainable model\n",
    "    \n",
    "\n",
    "- In real time situations (predominantly random, unpredictable events), this mapping includes a component of error / noise in the data; the actual model is $Y=f(X)+\\textrm{Error}(\\epsilon)$.\n",
    "    \n",
    "\n",
    "- If we assume, $k$ is the number of predictors then the form of Linear model is \n",
    "$$E[Y|X] = \\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdot\\cdot\\cdot\\cdot+\\beta_kX_k+\\epsilon$$, where\n",
    "$\\beta's$ are **weights / coefficients / betas** that are to be **estimated** (not  to **determine**) from the data\n",
    "Errors $(\\epsilon)$ are random and assumed to follow a Normal distribution with mean zero and variance $\\sigma^2$\n",
    "    \n",
    "\n",
    "- Methods to build LM (and other models) is **highly dependent on the nature of the response variable**. Here, we assume the response variable as numeric (measurable, continuous) one\n",
    "\n",
    "<hr style=\"border: 1.5px solid blue\"/>\n",
    "\n",
    "#### Necessary Symbols:\n",
    "\n",
    "- k: Number of predictors\n",
    "    \n",
    "    \n",
    "- p = k + 1 (number of predictors + constant term)\n",
    "    \n",
    "    \n",
    "- X: The model matrix of predictors, including the constant term, is represented as:  \n",
    "$$X = \\begin{pmatrix}\n",
    "  x_{ij} \\end{pmatrix}_{n\\times p}$$  \n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "  x_{10} & x_{11} & \\cdots & x_{1k} \\\\\n",
    "  x_{20} & x_{21} & \\cdots & x_{2k} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  x_{n0} & x_{n1} & \\cdots & x_{nk}\n",
    "\\end{pmatrix}_{n \\times p}\n",
    "$$ where $x_{i0} = 1 ~~\\forall i = 1,2\\cdots n$, representing the intercept term (constant term) in the linear model\n",
    "and $x_{ij} $ for $j = 1, 2, 3, \\dots, k$ are the values of the predictor variables for each of the $n$ data points.\n",
    "\n",
    "    \n",
    "\n",
    "- Y: Response variable \n",
    "$$Y =\n",
    " \\begin{pmatrix}\n",
    "  y_{1} & y_{2} & \\cdots & y_{n} \n",
    " \\end{pmatrix}^T_{n\\times 1}$$\n",
    "\n",
    "    \n",
    "- Mean Response $\\mu = E[Y]$\n",
    "$$\\mu =\n",
    " \\begin{pmatrix}\n",
    "  \\mu_{1} & \\mu_{2} & \\cdots & \\mu_{n} \n",
    " \\end{pmatrix}^T_{n\\times 1}$$ with $\\mu_i=E[y_i]$\n",
    " \n",
    "    \n",
    "- $\\beta$: Parameter matrix \n",
    " $$\\beta =\n",
    " \\begin{pmatrix}\n",
    "  \\beta_{1} & \\beta_{2} &   \\cdots  & \\beta_{p} \n",
    " \\end{pmatrix}^T_{p\\times 1}$$ \n",
    " \n",
    "    \n",
    "- $\\epsilon$: Random error matrix. The errors are assumed to follow Normal distribution with mean zero and unknown variance $\\sigma^2$; also, it is assumed that the errors are uncorrelated.\n",
    " $$\\epsilon =\n",
    " \\begin{pmatrix}\n",
    "  \\epsilon_{1} &  \\epsilon_{2} &  \\cdots & \\epsilon_{n} \n",
    " \\end{pmatrix}^T_{n\\times 1}$$ \n",
    "\n",
    "\n",
    "    \n",
    "A Linear Model is $$Y_{n \\times 1}=X_{n \\times p}\\beta\\,+\\,\\epsilon_{n \\times 1}$$\n",
    "    \n",
    "$$ \\implies \n",
    " \\begin{pmatrix}\n",
    "  y_{1} \\\\\n",
    "  y_{2} \\\\\n",
    "  \\vdots  \\\\\n",
    "  y_{n} \n",
    " \\end{pmatrix}_{n\\times 1}=\\begin{pmatrix}\n",
    "  x_{10} & x_{11} & \\cdots & x_{1k} \\\\\n",
    "  x_{20} & x_{21} & \\cdots & x_{2k} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  x_{n0} & x_{n1} & \\cdots & x_{nk}\n",
    "\\end{pmatrix}_{n \\times p}\\,\n",
    " \\begin{pmatrix}\n",
    "  \\beta_{0} \\\\\n",
    "  \\beta_{1} \\\\\n",
    "  \\vdots  \\\\\n",
    "  \\beta_{k} \n",
    " \\end{pmatrix}_{p\\times 1}+\\begin{pmatrix}\n",
    "  \\epsilon_{1} \\\\\n",
    "  \\epsilon_{2} \\\\\n",
    "  \\vdots  \\\\\n",
    "  \\epsilon_{n} \n",
    " \\end{pmatrix}_{n\\times 1}$$ \n",
    " \n",
    " $$=\\begin{pmatrix}\n",
    " x_{10}\\beta_{0}+x_{11}\\beta_{1}+x_{12}\\beta_{2}+\\cdots\\cdots x_{1k}\\beta_{k}\\\\\n",
    " x_{20}\\beta_{0}+x_{21}\\beta_{1}+x_{22}\\beta_{2}+\\cdots\\cdots x_{2k}\\beta_{k}\\\\\n",
    " \\vdots \\\\\n",
    " x_{n0}\\beta_{0}+x_{n1}\\beta_{1}+x_{n2}\\beta_{2}+\\cdots\\cdots x_{nk}\\beta_{k}\n",
    " \\end{pmatrix}_{n \\times 1}+\\begin{pmatrix}\n",
    "  \\epsilon_{1} \\\\\n",
    "  \\epsilon_{2} \\\\\n",
    "  \\vdots  \\\\\n",
    "  \\epsilon_{n} \n",
    " \\end{pmatrix}_{n\\times 1}$$\n",
    " \n",
    " $$=\\begin{pmatrix}\n",
    " \\beta_{0}+x_{11}\\beta_{1}+x_{12}\\beta_{2}+\\cdots\\cdots x_{1k}\\beta_{k}+\\epsilon_1\\\\\n",
    " \\beta_{0}+x_{21}\\beta_{1}+x_{22}\\beta_{2}+\\cdots\\cdots x_{2k}\\beta_{k}+\\epsilon_2\\\\\n",
    " \\vdots \\\\\n",
    " \\beta_{0}+x_{n1}\\beta_{1}+x_{n2}\\beta_{2}+\\cdots\\cdots x_{nk}\\beta_{k}+\\epsilon_n\n",
    " \\end{pmatrix}_{n \\times 1}$$ since $x_{i0}=1 \\,\\, \\forall i = 1,2,3,\\cdots\\cdots n$\n",
    "\n",
    "\n",
    "$$\\implies \\mu = E(Y|X) = \\sum_{j=0}^k \\beta_j\\,x_{ij}$$ and $$ V(Y|X)=\\sigma^2 $$. \n",
    "    \n",
    "Hence, in the linear model, we estimate the mean of the response variable $Y$ as a linear function of the predictors \n",
    "$X$, and not the exact value of $Y$\n",
    "\n",
    "<hr style=\"border: 1.5px solid blue\"/>\n",
    "    \n",
    "## <font color = darkgreen> Aim: Parameter estimation ($\\hat \\beta$) \n",
    "    \n",
    "The notion of Least Square Method (LSM) is to minimize the value of $||Y-\\hat \\mu||^2$\n",
    "\n",
    "\n",
    "$$||Y-\\hat \\mu||^2 = \\sum_{i=1}^{n}(y_i-\\hat\\mu_i)^2= \\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}\\hat\\beta_jx_{ij})^2$$\n",
    "\n",
    "Similar expression can be obtained from the notion of MLE when Our assumption is assume that $\\forall i = 1,2,\\cdots n~~~~ y_i\\sim \\text{Normal}(\\mu_i,\\sigma^2)$. \n",
    "\n",
    "$\\Rightarrow$ the log likelihood (excluding constant) is $$l(\\beta|y) = -\\frac{\\sum_i (y_i-\\mu_i)^2}{2\\sigma^2}$$. Hence, maximizing likelihood is equivalent to minimize the $\\sum_i (y_i-\\mu_i)^2$\n",
    "\n",
    "Hence $l(\\beta) = \\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}\\beta_jx_{ij})^2$ yields p equations while we equate $\\frac{\\partial~ l }{\\partial~ \\beta_j}$ to zero $\\forall j=1,2,\\cdots p$\n",
    "\n",
    "That is $\\sum_i(y_i-\\mu_i)x_{ij}=0 ~~~~~\\forall j=1,2,\\cdots p$\n",
    "\n",
    "Hence the LS estimates satisfy $\\sum_iy_ix_{ij}=\\sum_i\\hat\\mu_ix_{ij}~~~~~\\forall j=1,2,\\cdots p$\n",
    "\n",
    "#### Matrix Notation \n",
    "\n",
    "$l(\\beta)=||y-X\\beta||^2=(y-X\\beta)^T(y-X\\beta)$\n",
    "\n",
    "=$(y^T-\\beta^TX^T)(y-X\\beta)$\n",
    "\n",
    "=$y^Ty-y^TX\\beta-\\beta^TX^Ty+\\beta^TX^TX\\beta$. It can be observed that $y^TX\\beta$ is a constant and hence $\\beta^TX^Ty=(y^TX\\beta)^T=y^TX\\beta$\n",
    "\n",
    "Hence, $l(\\beta)=y^Ty-2y^TX\\beta+\\beta^TX^TX\\beta$\n",
    "\n",
    "#### Results: $\\frac{\\partial (a^T\\beta)}{\\partial\\beta}=a$ and $\\frac{\\partial (\\beta^TA\\beta)}{\\partial\\beta}=(A+A^T)\\beta=2A\\beta ~~~~~\\text{(if A is symmetric)}$\n",
    "\n",
    "Using these results $\\frac{\\partial ~l}{\\partial\\beta}=-2X^Ty+2X^TX\\beta=-2X^T(y-X\\beta)$\n",
    "\n",
    "Hence, $\\frac{\\partial ~l}{\\partial\\beta}=0 \\Rightarrow X^TX\\beta=X^Ty$\n",
    "\n",
    "$\\Rightarrow \\hat\\beta= (X^TX)^{-1}X^Ty$\n",
    "\n",
    "Also, $\\frac{\\partial^2 ~l}{\\partial\\beta^2}=2X^TX$ which is positive definite and hence the minimum exists at $\\hat\\beta$\n",
    "\n",
    "\n",
    "Our assumption is $Y\\sim \\text{Normal}(\\mu,\\sigma^2)$.\n",
    "\n",
    "    \n",
    "$\\therefore$ its pdf is $$ f(Y)=\\frac{1}{\\sigma\\,\\sqrt{2\\pi}}\\,e^{-\\frac{(Y-\\mu)^2}{2\\sigma^2}}$$ where $\\mu_i=\\sum_{j=0}^k \\beta_j\\,x_{ij}$ or in matrix notation, $\\mu= X\\beta$ \n",
    "    \n",
    "$\\Rightarrow$ the log likelihood (excluding constant) is $$l(\\beta|Y) = -\\frac{\\sum_i (y_i-\\mu_i)^2}{2\\sigma^2}$$\n",
    "    \n",
    "$$= \\sum_{i=1}^{n}(y_i-\\sum_{j=0}^{k}\\beta_j x_{ij})^2$$\n",
    "    \n",
    "Hence, to maximize $l(\\beta|Y)$, it is enough to minimize $\\frac{\\sum_i (y_i-\\mu_i)^2}{2\\sigma^2}$. This is because maximizing a function f(x) is equivalent to minimizing its negative, âˆ’f(x)\n",
    "\n",
    "In matrix notation, \n",
    "\n",
    "$l(\\beta)=||Y-X\\beta||^2=(Y-X\\beta)^T(Y-X\\beta)$\n",
    "\n",
    "=$(Y^T-\\beta^TX^T)(Y-X\\beta)$\n",
    "\n",
    "=$Y^TY-Y^TX\\beta-\\beta^TX^TY+\\beta^TX^TX\\beta$. It can be observed that $Y^TX\\beta$ is a scalar, A matrix of order $1\\times 1$. \n",
    "    \n",
    "Furthermore $\\beta^TX^TY=(Y^TX\\beta)^T$, and transpose of a scalar is the same scalar. This implies that, $-Y^TX\\beta-\\beta^TX^TY=-2Y^TX\\beta$\n",
    "\n",
    "Hence, $l(\\beta)=Y^TY-2Y^TX\\beta+\\beta^TX^TX\\beta$\n",
    "    \n",
    "Now let us use two results from matrix differentiation,\n",
    "    \n",
    "1. $\\frac{\\partial (a^T\\beta)}{\\partial\\beta}=a$ \n",
    "    \n",
    "1. $\\frac{\\partial (\\beta^TA\\beta)}{\\partial\\beta}=(A+A^T)\\beta=2A\\beta ~~~~~\\text{(if A is symmetric)}$\n",
    "    \n",
    "Using these results $\\frac{\\partial ~l}{\\partial\\beta}=-2X^TY+2X^TX\\beta$\n",
    "\n",
    "To minimize, we need to equate the first derivative to zero\n",
    "\n",
    "Hence, $\\frac{\\partial ~l}{\\partial\\beta}=0 \\Rightarrow X^TX\\beta=X^TY$\n",
    "\n",
    "$\\Rightarrow \\hat\\beta= (X^TX)^{-1}X^TY$ provided the inverse of the symmatrix matrix $(X^TX)_{p \\times p}$\n",
    "\n",
    "Also, $\\frac{\\partial^2 ~l}{\\partial\\beta^2}=2X^TX$ which is positive a definite matrix (a square matrix $A$ for which all its eigenvalues are positive, and columns of $A$ are linearly independent)\n",
    "\n",
    "\n",
    "Hence the minimum exists at $\\hat\\beta = (X^TX)^{-1}X^TY$\n",
    "    \n",
    "This gives a closed form solution for the linear model in the matrix form.\n",
    "    \n",
    "## <font color = darkgreen> Summary - OLS Procedure with Cost Function\n",
    "\n",
    "Given the linear regression model:\n",
    "\n",
    "$$\n",
    "Y_{n \\times 1} = X_{n \\times p} \\beta + \\epsilon_{n \\times 1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $Y$ is the $n \\times 1$ vector of observed response values.\n",
    "- $X$ is the $n \\times p$ matrix of predictors (independent variables).\n",
    "- $\\beta$ is the $p \\times 1$ vector of unknown regression coefficients.\n",
    "- $\\epsilon$ is the $n \\times 1$ vector of errors (residuals).\n",
    "\n",
    "### <font color = darkblue> Objective: Minimize the Cost Function\n",
    "\n",
    "The goal of **Ordinary Least Squares (OLS)** is to estimate the coefficients $\\beta$ by minimizing the **cost function**, which is the sum of squared residuals (errors). The residuals are the differences between the observed values $Y$ and the predicted values $X\\beta$.\n",
    "\n",
    "The **cost function** $J(\\beta)$ is given by:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\| Y - X\\beta \\|^2\n",
    "$$\n",
    "\n",
    "This represents the sum of squared residuals, and we seek to minimize this function in order to find the best-fitting coefficients $\\beta$.\n",
    "\n",
    "### Solution\n",
    "\n",
    "To minimize the cost function, we take the derivative of $J(\\beta)$ with respect to $\\beta$, set it equal to zero, and solve for $\\beta$. The closed-form solution for the estimated coefficients is:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "### <font color = darkblue> Assumptions\n",
    "\n",
    "- The matrix $X^T X$ must be **invertible** for the solution to exist. This requires that $X$ has **full column rank** (i.e., the predictors must be linearly independent).\n",
    "- If $X^T X$ is not invertible (e.g., if some columns of $X$ are linearly dependent), the solution does not exist, or the matrix may only be positive semi-definite.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <font color = darkblue> Key Points:\n",
    "1. The cost function is minimized to estimate the regression coefficients $\\beta$.\n",
    "\n",
    "2. The solution is derived by solving for $\\beta$ in the closed-form equation $\\hat{\\beta} = (X^T X)^{-1} X^T Y$.\n",
    "\n",
    "3. The matrix $X^T X$ must be invertible, which requires that the columns of $X$ are linearly independent.\n",
    "    \n",
    "4. OLS estimates the coefficients $\\beta$ by minimizing the **cost function** $J(\\beta) = \\| Y - X\\beta \\|^2$. The solution to this minimization problem is given by the closed-form equation:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "This formula provides the best-fit linear regression coefficients, assuming that $X^T X$ is invertible and that $X$ has full column rank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = maroon> Implementation of OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5. 19.]\n",
      " [19. 87.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data points\n",
    "xd1 = np.array([1, 5, 6, 4, 3]).reshape(5, 1)\n",
    "\n",
    "# Number of data points, Number of regressors\n",
    "nd, ni = xd1.shape\n",
    "p = ni + 1\n",
    "xd = xd1\n",
    "\n",
    "# One's created as a column matrix\n",
    "xo = np.ones((nd, 1))\n",
    "\n",
    "# Coefficients matrix\n",
    "xm = xd\n",
    "\n",
    "# Modal matrix (X matrix with column of ones prepended)\n",
    "x = np.hstack((xo, xm))\n",
    "\n",
    "# X'X\n",
    "m = np.dot(x.T, x)\n",
    "\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.99999999999997\n",
      "Matrix X'X is invertible\n",
      "[[ 1.17567568 -0.25675676]\n",
      " [-0.25675676  0.06756757]]\n",
      "Intercept (beta_0): [1.8514]\n",
      "Variable coefficients beta_1: [[0.9865]]\n"
     ]
    }
   ],
   "source": [
    "# Determinant of m to check singularity\n",
    "d = np.linalg.det(m)\n",
    "print(d)\n",
    "\n",
    "if d == 0:\n",
    "    print(\"Matrix X'X is singular\")\n",
    "else:\n",
    "    print(\"Matrix X'X is invertible\")\n",
    "\n",
    "# Response variable\n",
    "y = np.array([4, 6, 10, 3, 5]).reshape(5, 1)\n",
    "\n",
    "# OLS - Ordinary Least Squares\n",
    "if d != 0:\n",
    "    mi = np.linalg.inv(m)    # Inverse of X'X\n",
    "        \n",
    "print(mi)\n",
    "\n",
    "# Regression beta coefficients\n",
    "betas = np.dot(mi, np.dot(x.T, y))\n",
    "intercept = np.round(betas[0],4)  # The first value is the intercept (beta_0)\n",
    "beta_values = np.round(betas[1:] ,4) # The second value is beta_1\n",
    "\n",
    "print(\"Intercept (beta_0):\", intercept)\n",
    "print(\"Variable coefficients beta_1:\", beta_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue> Gradient Descent (GD) Algorithm for Linear Model Estimation\n",
    "\n",
    "### <font color = green> What is Gradient Descent?\n",
    "\n",
    "**Gradient Descent (GD)** is an iterative optimization algorithm used to minimize a cost function. It is commonly used when the closed-form solution to the optimization problem is difficult or computationally expensive to obtain.\n",
    "\n",
    "In the context of **Linear Models (LM)**, we typically use Gradient Descent to find the optimal parameters (or coefficients) $\\beta$ that minimize the **cost function**, which is often the **sum of squared residuals** \n",
    "\n",
    "Given the linear model:\n",
    "\n",
    "$$\n",
    "Y_{n \\times 1} = X_{n \\times p} \\beta + \\epsilon_{n \\times 1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $Y$ is the $n \\times 1$ vector of observed response values.\n",
    "- $X$ is the $n \\times p$ matrix of predictors (independent variables).\n",
    "- $\\beta$ is the $p \\times 1$ vector of regression coefficients (the parameters to be estimated).\n",
    "- $\\epsilon$ is the $n \\times 1$ vector of errors (residuals).\n",
    "\n",
    "We aim to estimate $\\beta$ by minimizing the **cost function**:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\| Y - X\\beta \\|^2\n",
    "$$\n",
    "\n",
    "### <font color = green> How Gradient Descent Works\n",
    "\n",
    "The idea behind Gradient Descent is to iteratively adjust the parameters $\\beta$ in the direction that reduces the value of the cost function. The algorithm works by computing the **gradient** (or partial derivatives) of the cost function with respect to $\\beta$, and then updating $\\beta$ in the opposite direction of the gradient.\n",
    "\n",
    "The **update rule** for Gradient Descent is:\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\nabla_{\\beta} J(\\beta^{(t)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\beta^{(t)}$ is the vector of parameters at iteration $t$.\n",
    "- $\\alpha$ is the **learning rate**, a positive scalar that controls the step size of each update.\n",
    "- $\\nabla_{\\beta} J(\\beta^{(t)})$ is the gradient of the cost function $J(\\beta)$ with respect to $\\beta$ at iteration $t$.\n",
    "\n",
    "The gradient of the cost function with respect to $\\beta$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} J(\\beta) = -2 X^T (Y - X\\beta)\n",
    "$$\n",
    "\n",
    "Thus, the update rule becomes:\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} + 2\\alpha X^T (Y - X\\beta^{(t)})\n",
    "$$\n",
    "\n",
    "### <font color = green> Gradient Descent for Linear Model Estimation\n",
    "\n",
    "1. **Initialization**: Start with an initial guess for $\\beta^{(0)}$. This can be a vector of zeros or some other small random values.\n",
    "\n",
    "2. **Iterative Updates**: At each iteration, update the value of $\\beta$ using the gradient of the cost function. The update depends on the current value of $\\beta$ and the **learning rate** $\\alpha$.\n",
    "\n",
    "3. **Convergence**: The algorithm continues iterating until the change in the cost function $J(\\beta)$ is sufficiently small, indicating convergence. This means that the algorithm has found a solution where the cost function is near its minimum.\n",
    "\n",
    "### <font color = green> Advantages of Gradient Descent in Linear Model Estimation\n",
    "\n",
    "1. **Scalability**: Gradient Descent is particularly useful when dealing with large datasets, as it does not require the computation of the inverse of $X^T X$, which can be computationally expensive for large $n$ and $p$. Unlike the closed-form OLS solution, which requires $(X^T X)^{-1}$, Gradient Descent is an iterative process and can be applied to problems with many features or large datasets.\n",
    "\n",
    "2. **Flexibility**: It can be used with different cost functions and regularization techniques (e.g., **Ridge Regression** or **Lasso**) where the closed-form solution might not exist.\n",
    "\n",
    "3. **Handling Non-linearity**: While OLS assumes a linear model, Gradient Descent can be extended to more complex models that may not have closed-form solutions, such as neural networks and other machine learning algorithms.\n",
    "\n",
    "### <font color = green> Key Considerations for Gradient Descent\n",
    "\n",
    "- **Learning Rate**: The choice of the learning rate $\\alpha$ is crucial. If $\\alpha$ is too large, the algorithm may overshoot the optimal solution and fail to converge. If $\\alpha$ is too small, the algorithm may converge very slowly. Therefore, selecting an appropriate learning rate is important.\n",
    "  \n",
    "- **Convergence**: In practice, Gradient Descent may converge to a local minimum or require many iterations to converge, especially in high-dimensional settings. Techniques like **stochastic gradient descent** (SGD) and **mini-batch gradient descent** can help in speeding up convergence in large datasets.\n",
    "\n",
    "### <font color = green> Summary\n",
    "\n",
    "- **Gradient Descent (GD)** is an optimization algorithm used to find the optimal coefficients $\\beta$ in a linear regression model.\n",
    "\n",
    "- It iteratively updates $\\beta$ by computing the gradient of the cost function with respect to $\\beta$ and adjusting the parameters in the direction that minimizes the cost.\n",
    "\n",
    "- GD is useful in scenarios where the closed-form solution for linear regression (OLS) is computationally expensive or infeasible, especially with large datasets or high-dimensional problems.\n",
    "\n",
    "\n",
    "### <font color = green> Key Points:\n",
    "\n",
    "- **Gradient Descent** is used to minimize the **cost function** \\( J(\\beta) = \\| Y - X\\beta \\|^2 \\) iteratively.\n",
    "\n",
    "- **Iterative Updates**: The parameters \\( \\beta \\) are updated by moving in the direction opposite to the gradient of the cost function.\n",
    "\n",
    "- **Advantages**: GD is particularly helpful for large datasets or models that are difficult to solve with a closed-form solution (like OLS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue> Comparison of **OLS (Ordinary Least Squares)** and **Gradient Descent (GD)**\n",
    "\n",
    "Both **OLS** and **Gradient Descent (GD)** are used to estimate the coefficients ($\\beta$) in linear regression models, but they approach the problem in different ways. Below is a detailed comparison:\n",
    "\n",
    "\n",
    "\n",
    "### <font color = green> 1. **Methodology**\n",
    "\n",
    "- **OLS (Ordinary Least Squares)**:\n",
    "  - **Closed-form solution**: OLS provides a direct, closed-form solution for the coefficients $\\beta$.\n",
    "  - The formula for the coefficients is:\n",
    "    \\[\n",
    "    \\hat{\\beta} = (X^T X)^{-1} X^T Y\n",
    "    \\]\n",
    "  - The solution is derived by minimizing the sum of squared residuals  using matrix algebra.\n",
    "  \n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Iterative optimization**: GD is an iterative algorithm that estimates the coefficients $\\beta$ by continuously adjusting them to minimize the cost function (sum of squared residuals).\n",
    "  - The update rule for GD is:\n",
    "    \\[\n",
    "    \\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\nabla_{\\beta} J(\\beta^{(t)})\n",
    "    \\]\n",
    "  - It uses the gradient of the cost function to iteratively move towards the optimal solution.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 2. **Computation**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Direct Calculation**: OLS directly calculates $\\hat{\\beta}$ in one step by solving the matrix equation. This involves computing $ (X^T X)^{-1} $, which can be computationally expensive, especially for large datasets.\n",
    "  - Requires inversion of the matrix $X^T X$, which can be problematic if $X^T X$ is singular (non-invertible).\n",
    "\n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Iterative Calculation**: GD doesn't require matrix inversion. Instead, it works iteratively, updating the coefficients in small steps based on the gradient of the cost function.\n",
    "  - It can work with large datasets and high-dimensional features since it doesn't involve matrix inversion.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 3. **Convergence and Speed**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Faster for small to medium datasets**: OLS is efficient for small to medium-sized datasets, as the closed-form solution is quick to compute.\n",
    "  - **No iterations**: Since OLS provides a direct solution, it does not require iterations or hyperparameter tuning.\n",
    "  \n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Slower and iterative**: GD can be slower, as it requires multiple iterations to converge to the optimal solution. The speed of convergence depends on factors such as the learning rate ($\\alpha$) and the number of iterations.\n",
    "  - **Can be computationally expensive for large datasets**: Each iteration involves computing the gradient, which can be costly for very large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 4. **Scalability**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Limited scalability**: OLS can be inefficient for very large datasets, especially when the matrix $X^T X$ is large and inversion becomes computationally expensive.\n",
    "  - May not work well with high-dimensional data (large $p$) or when $X^T X$ is singular or nearly singular.\n",
    "\n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Highly scalable**: GD is more scalable because it does not require matrix inversion. It can be applied to large datasets and high-dimensional problems.\n",
    "  - Can also be applied to **mini-batch gradient descent** or **stochastic gradient descent (SGD)**, which can speed up the convergence by processing small subsets (batches) of data.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 5. **Requirements and Assumptions**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Assumes full rank**: OLS assumes that the matrix $X^T X$ is invertible, which requires $X$ to have full column rank (i.e., the columns of $X$ should be linearly independent).\n",
    "  - Works best when the matrix is not too large or singular.\n",
    "  \n",
    "- **Gradient Descent (GD)**:\n",
    "  - **No assumption of invertibility**: GD does not require the matrix $X^T X$ to be invertible. It works iteratively and can handle situations where OLS may fail (e.g., with singular matrices).\n",
    "  - However, it does require selecting an appropriate **learning rate** ($\\alpha$) to ensure convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 6. **Handling Large and Complex Data**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Not ideal for large datasets**: OLS is not ideal for very large datasets because the matrix inversion $ (X^T X)^{-1} $ becomes computationally expensive and memory-intensive.\n",
    "  - May struggle with high-dimensional data when $p$ (number of predictors) is large relative to $n$ (number of observations).\n",
    "\n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Ideal for large datasets**: GD is well-suited for large datasets and high-dimensional problems because it processes data in small batches (in the case of stochastic gradient descent).\n",
    "  - Can be applied to datasets where the number of features $p$ is much larger than the number of observations $n$.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 7. **Precision and Accuracy**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Exact solution**: OLS provides an exact solution (assuming the conditions for invertibility hold). It guarantees that the coefficients found are the best fit for minimizing the sum of squared residuals, as long as the matrix $X^T X$ is invertible.\n",
    "\n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Approximate solution**: GD provides an approximate solution, and the precision depends on the number of iterations and the learning rate $\\alpha$.\n",
    "  - May converge to a local minimum or take longer to converge, depending on the starting point and the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color = green> 8. **Robustness**\n",
    "\n",
    "- **OLS**:\n",
    "  - **Sensitive to outliers**: OLS is sensitive to outliers because it minimizes the sum of squared residuals, which can be disproportionately influenced by large residuals.\n",
    "  \n",
    "- **Gradient Descent (GD)**:\n",
    "  - **Can be made more robust**: GD can be made more robust by using regularization techniques (like Ridge or Lasso) or by applying more sophisticated variants like **stochastic gradient descent (SGD)** with learning rate schedules.\n",
    "\n",
    "### <font color = green> 9. **Algorithmic Complexity**\n",
    "\n",
    "- **OLS (Ordinary Least Squares)**:\n",
    "  - The closed-form solution of OLS involves matrix operations like matrix multiplication and inversion. The time complexity for calculating $ \\hat{\\beta} = (X^T X)^{-1} X^T Y $ is dominated by the matrix inversion step, which is $ O(p^3) $, where $ p $ is the number of predictors (features).\n",
    "  - The complexity of matrix multiplication $ X^T X $ is $ O(np^2) $, and the inversion of $ X^T X $ takes $ O(p^3) $.\n",
    "  - **Total Time Complexity**: $ O(np^2 + p^3) $.\n",
    "\n",
    "- **Gradient Descent (GD)**:\n",
    "  \n",
    "    - In each iteration, the time complexity of computing the gradient $ \\nabla_{\\beta} J(\\beta) = -2 X^T (Y - X\\beta) $ is $ O(np) $, as it involves multiplying the $ n \\times p $ matrix $ X^T $ with the residual vector $ Y - X\\beta $.\n",
    "  \n",
    "    - The number of iterations required for convergence depends on factors like the learning rate and convergence criteria. If we assume $ T $ iterations to reach convergence, the time complexity of GD is $ O(T \\cdot np) $.\n",
    "  \n",
    "    - **Total Time Complexity**: $ O(Tnp) $, where $ T $ is the number of iterations.\n",
    "\n",
    "\n",
    "\n",
    "### Comparison Table \n",
    "\n",
    "| Feature                  | OLS (Ordinary Least Squares)                  | Gradient Descent (GD)                           |\n",
    "|--------------------------|-----------------------------------------------|-------------------------------------------------|\n",
    "| **Solution Type**         | Closed-form solution                          | Iterative solution                              |\n",
    "| **Computation**           | Requires matrix inversion                     | Iterative updates (gradient computation)        |\n",
    "| **Speed**                 | Faster for small datasets, no iterations      | Slower, but can be controlled by learning rate |\n",
    "| **Scalability**           | Limited to small/medium datasets              | Highly scalable, works for large datasets       |\n",
    "| **Assumptions**           | Full rank of $ X^T X $, invertibility       | No assumptions about matrix invertibility       |\n",
    "| **Handling Large Data**   | Inefficient for very large data               | Efficient, handles large datasets well          |\n",
    "| **Precision**             | Exact solution (if conditions hold)           | Approximate solution, depends on iteration      |\n",
    "| **Algorithmic Complexity**| $ O(np^2 + p^3) $                           | $ O(Tnp) $, where $ T $ is the number of iterations |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "- **OLS** is fast and provides an exact solution when the design matrix $X$ is not too large and is invertible, making it ideal for small to medium-sized datasets. It is computationally expensive when the number of features $ k $is large or when the dataset $ n $ is large, due to the $ O(p^3) $ complexity from matrix inversion.\n",
    "\n",
    "- **Gradient Descent** is more scalable and suitable for large datasets or high-dimensional problems. The time complexity depends on the number of iterations, but the per-iteration cost is generally $ O(np)$, which is more efficient than OLS for large-scale problems. However, the convergence of GD can be slower, and the number of iterations needed depends on factors like the learning rate and data structure."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVaDYtmoZogDYzk6ShA0F9",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
